{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "from keras.layers import Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as ply\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"D:\\\\My Personal Documents\\\\Learnings\\\\Data Science\\\\Data Sets\\\\Twiter Sentiment Analysis\\\\train_E6oV3lV.csv\"\n",
    "test = 'D:\\\\My Personal Documents\\\\Learnings\\\\Data Science\\\\Data Sets\\\\Twiter Sentiment Analysis\\\\test_tweets_anuFYb8.csv'\n",
    "tweets = pd.read_csv(file)\n",
    "test_tweets=pd.read_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       2242\n",
      "label    2242\n",
      "tweet    2242\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tweets1=tweets[tweets.label==1]\n",
    "print(tweets1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets.append(tweets1)\n",
    "tweets=tweets.append(tweets1)\n",
    "tweets=tweets.append(tweets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=tweets.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       8968\n",
       "label    8968\n",
       "tweet    8968\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets.label==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "tweets=tweets.append(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    29720\n",
       "1.0     8968\n",
       "Name: tweet, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(tweets.label).tweet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = tweets.tweet.str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet=cleaned_tweet.apply(lambda x :\" \".join(w.lower() for w in x.split() if  w != 'user' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = cleaned_tweet.apply(lambda x : \" \".join( stem.stem(w) for w in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = cleaned_tweet.apply(lambda x : \" \".join( w for w in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=9300,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=tfidf.fit_transform(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = bow[:38688,:]\n",
    "test_bow = bow[38688:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train, y_test = train_test_split(train_bow,label,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990695270095632\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model_svm= svm.SVC(kernel='rbf', C=1,gamma=1)\n",
    "model_svm.fit(x_train,y_train)\n",
    "\n",
    "pred_svm=model_svm.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(metrics.accuracy_score(y_test,pred_svm))\n",
    "\n",
    "test_pred_svm= model_svm.predict(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9558025329542518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_NB= MultinomialNB(0.25)\n",
    "model_NB.fit(x_train,y_train)\n",
    "pred_NB=model_NB.predict(x_test)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(metrics.accuracy_score(y_test,pred_NB))\n",
    "\n",
    "test_pred_NB= model_NB.predict(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR= LogisticRegression()\n",
    "model_LR.fit(x_train,y_train)\n",
    "test_pred_LR= model_LR.predict(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9784181959162574\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "model_xgb = XGBClassifier( nthread=4, n_estimators=200, learning_rate=0.2,\n",
    "       colsample_bytree=0.85,\n",
    "        max_depth=50,\n",
    "        reg_lambda=1,\n",
    "min_split_gain=0.0222415)\n",
    "model_xgb.fit(x_train,y_train)\n",
    "pred_xgb=model_xgb.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(metrics.accuracy_score(y_test,pred_xgb))\n",
    "\n",
    "test_pred_xgb= model_xgb.predict(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30950 samples, validate on 7738 samples\n",
      "Epoch 1/1\n",
      "30950/30950 [==============================] - 489s 16ms/step - loss: 0.1635 - acc: 0.9358 - val_loss: 0.0785 - val_acc: 0.9676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba16ff7400>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "output_dir = 'model_tweet_output/dense'\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "x_train,x_test,y_train, y_test = train_test_split(train_bow,label,test_size=0.2,random_state=3)\n",
    "\n",
    "model_mlp_tfidf = Sequential()\n",
    "model_mlp_tfidf.add(Dense(3000,activation='relu', input_shape=(9300,)))\n",
    "model_mlp_tfidf.add(Dropout(0.2))\n",
    "model_mlp_tfidf.add(Dense(1,activation='sigmoid'))\n",
    "model_mlp_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_mlp_tfidf.fit(x_train,y_train,epochs=3,verbose=1, validation_data=(x_test,y_test),callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_tfidf.load_weights(output_dir+'/weights.03.hdf5')\n",
    "test_pred_mlp_tfidf=model_mlp_tfidf.predict_classes(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM with Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_tweet)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_tweet)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "word_index=pad_sequences(sequences, maxlen=40, padding = 'pre', truncating='pre')\n",
    "\n",
    "train = word_index[:38688,:]\n",
    "test = word_index[38688:,:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train, y_test = train_test_split(train,label,test_size=0.2,random_state=3, stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30950 samples, validate on 7738 samples\n",
      "Epoch 1/1\n",
      "30950/30950 [==============================] - 57s 2ms/step - loss: 0.2383 - acc: 0.9043 - val_loss: 0.1230 - val_acc: 0.9536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba16e11630>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = 'model_tweet_output/LSTM'\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "n_dim = 64\n",
    "n_unique_words = 50000\n",
    "n_words_to_skip = 5\n",
    "max_review_length = 40\n",
    "pad_type = trunc_type = 'pre'\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(n_unique_words,n_dim, input_length=max_review_length))\n",
    "model_lstm.add(LSTM(30,return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(30))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_lstm.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test,y_test), callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.load_weights(output_dir+'/weights.05.hdf5')\n",
    "test_pred_lstm = model_lstm.predict_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_tweet_output/cnn'\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "n_dim = 128\n",
    "n_unique_words = 50000\n",
    "n_words_to_skip = 5\n",
    "max_review_length = 40\n",
    "pad_type = trunc_type = 'pre'\n",
    "drop_embed = 0.2\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.2\n",
    "\n",
    "n_conv = 256\n",
    "k_conv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30950 samples, validate on 7738 samples\n",
      "Epoch 1/1\n",
      "30950/30950 [==============================] - 504s 16ms/step - loss: 0.2529 - acc: 0.8939 - val_loss: 0.1055 - val_acc: 0.9592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba464665c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(n_unique_words,n_dim, input_length=max_review_length))\n",
    "model_cnn.add(SpatialDropout1D(drop_embed))\n",
    "model_cnn.add(Conv1D(n_conv, k_conv, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(n_dense,activation='relu'))\n",
    "model_cnn.add(Dropout(dropout))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_cnn.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test,y_test), callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.load_weights(output_dir+'/weights.03.hdf5')\n",
    "test_pred_cnn = model_cnn.predict_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_NB = pd.DataFrame(test_pred_NB)\n",
    "test_pred_svm = pd.DataFrame(test_pred_svm)\n",
    "test_pred_LR = pd.DataFrame(test_pred_LR)\n",
    "test_pred_xgb = pd.DataFrame(test_pred_xgb)\n",
    "test_pred_lstm = pd.DataFrame(test_pred_lstm)\n",
    "test_pred_cnn = pd.DataFrame(test_pred_cnn)\n",
    "test_pred_mlp_tfidf = pd.DataFrame(test_pred_mlp_tfidf)\n",
    "\n",
    "output = pd.concat([test_pred_NB,test_pred_svm,test_pred_LR,test_pred_xgb,test_pred_lstm,test_pred_cnn, test_pred_mlp_tfidf],axis=1)\n",
    "\n",
    "output1=output.mode(axis=1)\n",
    "output2=pd.concat([output1,test_pred_xgb,test_pred_mlp_tfidf,test_pred_NB,test_pred_lstm],axis=1).mode(axis=1)\n",
    "\n",
    "final_output=pd.DataFrame(pd.concat([test_tweets.id,output2], axis=1))\n",
    "final_output.columns=['id','label']\n",
    "final_output.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
