{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
    "from keras.layers import Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as ply\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"D:\\\\My Personal Documents\\\\Learnings\\\\Data Science\\\\Data Sets\\\\Twiter Sentiment Analysis\\\\train_E6oV3lV.csv\"\n",
    "test = 'D:\\\\My Personal Documents\\\\Learnings\\\\Data Science\\\\Data Sets\\\\Twiter Sentiment Analysis\\\\test_tweets_anuFYb8.csv'\n",
    "tweets = pd.read_csv(file)\n",
    "test_tweets=pd.read_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       2242\n",
      "label    2242\n",
      "tweet    2242\n",
      "dtype: int64\n",
      "id       29720\n",
      "label    29720\n",
      "tweet    29720\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tweets1=tweets[tweets.label==1]\n",
    "tweets0=tweets[tweets.label==0]\n",
    "print(tweets1.count())\n",
    "print(tweets0.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets0_undersample=tweets0.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets1\n",
    "tweets=tweets.append(tweets0_undersample)\n",
    "label=tweets.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.tweet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "tweets=tweets.append(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    8916\n",
       "1.0    2242\n",
       "Name: tweet, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(tweets.label).tweet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = tweets.tweet.str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet=cleaned_tweet.apply(lambda x :\" \".join(w.lower() for w in x.split() if  w != 'user' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "#from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = cleaned_tweet.apply(lambda x : \" \".join( stem.stem(w) for w in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = cleaned_tweet.apply(lambda x : \" \".join( w for w in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=3000,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=tfidf.fit_transform(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = bow[:11158,:]\n",
    "test_bow = bow[11158:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train, y_test = train_test_split(train_bow,label,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model_svm= svm.SVC(kernel='rbf', C=1,gamma=1)\n",
    "model_svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm=model_svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9126344086021505"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_svm= model_svm.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9099462365591398"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_NB= MultinomialNB(0.25)\n",
    "model_NB.fit(x_train,y_train)\n",
    "pred_NB=model_NB.predict(x_test)\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "metrics.accuracy_score(y_test,pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_NB= model_NB.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR= LogisticRegression()\n",
    "model_LR.fit(x_train,y_train)\n",
    "test_pred_LR= model_LR.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991935483870968"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "model_xgb = XGBClassifier( nthread=4, n_estimators=200, learning_rate=0.2,\n",
    "       colsample_bytree=0.85,\n",
    "        max_depth=50,\n",
    "        reg_lambda=1,\n",
    "min_split_gain=0.0222415)\n",
    "model_xgb.fit(x_train,y_train)\n",
    "pred_xgb=model_xgb.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "metrics.accuracy_score(y_test,pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_xgb= model_xgb.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "output_dir = 'model_tweet_output1/dense'\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8926 samples, validate on 2232 samples\n",
      "Epoch 1/3\n",
      "8926/8926 [==============================] - 19s 2ms/step - loss: 0.3241 - acc: 0.8605 - val_loss: 0.2268 - val_acc: 0.9108\n",
      "Epoch 2/3\n",
      "8926/8926 [==============================] - 18s 2ms/step - loss: 0.1539 - acc: 0.9386 - val_loss: 0.2320 - val_acc: 0.9091\n",
      "Epoch 3/3\n",
      "8926/8926 [==============================] - 20s 2ms/step - loss: 0.0995 - acc: 0.9639 - val_loss: 0.2640 - val_acc: 0.8992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x267aac46dd8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train, y_test = train_test_split(train_bow,label,test_size=0.2,random_state=3)\n",
    "\n",
    "model_mlp_tfidf = Sequential()\n",
    "model_mlp_tfidf.add(Dense(1000,activation='relu', input_shape=(3000,)))\n",
    "model_mlp_tfidf.add(Dropout(0.2))\n",
    "model_mlp_tfidf.add(Dense(1,activation='sigmoid'))\n",
    "model_mlp_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_mlp_tfidf.fit(x_train,y_train,epochs=3,verbose=1, validation_data=(x_test,y_test),callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_tfidf.load_weights(output_dir+'/weights.03.hdf5')\n",
    "test_pred_mlp_tfidf=model_mlp_tfidf.predict_classes(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_tweet)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_tweet)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "word_index=pad_sequences(sequences, maxlen=40, padding = 'pre', truncating='pre')\n",
    "\n",
    "train = word_index[:11158,:]\n",
    "test = word_index[11158:,:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train, y_test = train_test_split(train,label,test_size=0.2,random_state=3, stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8926 samples, validate on 2232 samples\n",
      "Epoch 1/5\n",
      "8926/8926 [==============================] - 15s 2ms/step - loss: 0.4151 - acc: 0.8306 - val_loss: 0.2631 - val_acc: 0.8893\n",
      "Epoch 2/5\n",
      "8926/8926 [==============================] - 15s 2ms/step - loss: 0.1687 - acc: 0.9361 - val_loss: 0.2286 - val_acc: 0.9113\n",
      "Epoch 3/5\n",
      "8926/8926 [==============================] - 13s 1ms/step - loss: 0.0870 - acc: 0.9724 - val_loss: 0.2512 - val_acc: 0.9122\n",
      "Epoch 4/5\n",
      "8926/8926 [==============================] - 13s 1ms/step - loss: 0.0509 - acc: 0.9842 - val_loss: 0.2932 - val_acc: 0.9091\n",
      "Epoch 5/5\n",
      "8926/8926 [==============================] - 14s 2ms/step - loss: 0.0284 - acc: 0.9917 - val_loss: 0.3479 - val_acc: 0.9095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x267c1c99748>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = 'model_tweet_output/LSTM'\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "n_dim = 64\n",
    "n_unique_words = 35000\n",
    "n_words_to_skip = 5\n",
    "max_review_length = 40\n",
    "pad_type = trunc_type = 'pre'\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(n_unique_words,n_dim, input_length=max_review_length))\n",
    "model_lstm.add(LSTM(30,return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(30))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_lstm.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test,y_test), callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.load_weights(output_dir+'/weights.03.hdf5')\n",
    "test_pred_lstm = model_lstm.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_tweet_output/cnn'\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "n_dim = 128\n",
    "n_unique_words = 35000\n",
    "n_words_to_skip = 5\n",
    "max_review_length = 40\n",
    "pad_type = trunc_type = 'pre'\n",
    "drop_embed = 0.2\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.2\n",
    "\n",
    "n_conv = 256\n",
    "k_conv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8926 samples, validate on 2232 samples\n",
      "Epoch 1/5\n",
      "8926/8926 [==============================] - 145s 16ms/step - loss: 0.4565 - acc: 0.7946 - val_loss: 0.3393 - val_acc: 0.8517\n",
      "Epoch 2/5\n",
      "8926/8926 [==============================] - 144s 16ms/step - loss: 0.2179 - acc: 0.9199 - val_loss: 0.2313 - val_acc: 0.9073\n",
      "Epoch 3/5\n",
      "8926/8926 [==============================] - 142s 16ms/step - loss: 0.0895 - acc: 0.9715 - val_loss: 0.2616 - val_acc: 0.9028\n",
      "Epoch 4/5\n",
      "8926/8926 [==============================] - 143s 16ms/step - loss: 0.0362 - acc: 0.9901 - val_loss: 0.3117 - val_acc: 0.9082\n",
      "Epoch 5/5\n",
      "8926/8926 [==============================] - 145s 16ms/step - loss: 0.0151 - acc: 0.9963 - val_loss: 0.3694 - val_acc: 0.9019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x267c4d2aa20>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(n_unique_words,n_dim, input_length=max_review_length))\n",
    "model_cnn.add(SpatialDropout1D(drop_embed))\n",
    "model_cnn.add(Conv1D(n_conv, k_conv, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(n_dense,activation='relu'))\n",
    "model_cnn.add(Dropout(dropout))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_cnn.fit(x_train,y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test,y_test), callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.load_weights(output_dir+'/weights.02.hdf5')\n",
    "test_pred_cnn = model_cnn.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_NB = pd.DataFrame(test_pred_NB)\n",
    "test_pred_svm = pd.DataFrame(test_pred_svm)\n",
    "test_pred_LR = pd.DataFrame(test_pred_LR)\n",
    "test_pred_xgb = pd.DataFrame(test_pred_xgb)\n",
    "test_pred_lstm = pd.DataFrame(test_pred_lstm)\n",
    "test_pred_cnn = pd.DataFrame(test_pred_cnn)\n",
    "test_pred_mlp_tfidf = pd.DataFrame(test_pred_mlp_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([test_pred_NB,test_pred_svm,test_pred_LR,test_pred_xgb,test_pred_lstm,test_pred_cnn, test_pred_mlp_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1=output.mode(axis=1)\n",
    "output2=pd.concat([output1,test_pred_xgb,test_pred_mlp_tfidf,test_pred_NB,test_pred_lstm],axis=1).mode(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output=pd.DataFrame(pd.concat([test_tweets.id,output2], axis=1))\n",
    "final_output.columns=['id','label']\n",
    "final_output.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
