{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_sensitive = pd.read_csv(r'C:\\NICE Documents\\Bank of Indonesia\\agg_data_nonsensitive.csv')\n",
    "data_sensitive = pd.read_csv(r'C:\\NICE Documents\\Bank of Indonesia\\agg_data_sensitive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_sensitive = data_non_sensitive.fillna(0)\n",
    "data_sensitive = data_sensitive.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_sensitive.rename(columns = {'#RIC': 'Currency_Pair'}, inplace=True)\n",
    "data_sensitive.rename(columns = {'#RIC': 'Currency_Pair'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, contamination_ratio):\n",
    "    df_stats = x_train\n",
    "    df_anomalies = pd.DataFrame([])\n",
    "    for currency_pair in list(x_train.Currency_Pair.unique()):\n",
    "        df_stats_original = df_stats[df_stats.Currency_Pair == currency_pair]\n",
    "        df_x = df_stats_original.drop(['Currency_Pair','Year','Month','Day','Hour'], axis=1)\n",
    "        df_x_original = df_x.copy()\n",
    "        row_count = df_x.shape[0]\n",
    "        #get the estimators\n",
    "        if row_count < 5:\n",
    "            estimaters = 1\n",
    "        elif row_count < 20:\n",
    "            estimaters = 5\n",
    "        elif row_count < 50:\n",
    "            estimaters = 10\n",
    "        elif row_count < 100:\n",
    "            estimaters = 30\n",
    "        elif row_count < 200:\n",
    "            estimaters = 50\n",
    "        elif row_count < 500:\n",
    "            estimaters = 100\n",
    "        else:\n",
    "            estimaters = 200\n",
    "        model = IsolationForest(n_estimators=200, contamination=1.0,  random_state=22)\n",
    "        model.fit(df_x)\n",
    "        #df_x['anomaly'] = pd.Series(model.predict(df_x_original)).values\n",
    "        #df_x['anomaly'] = df_x['anomaly'].map( {1: 0, -1: 1} )\n",
    "        #df_x_anomalous = df_x[df_x.anomaly==1].drop('anomaly',axis=1)\n",
    "        n = df_x_original.shape[0]\n",
    "        anomalous_data_index = pd.DataFrame(model.decision_function(df_x_original)).sort_values(by=0).index[0:n]\n",
    "        df_stats_original = df_stats_original.assign(Anomaly_Score=pd.Series(model.decision_function(df_x_original)).values)\n",
    "        df_anomalies_temp = df_stats_original.iloc[anomalous_data_index]\n",
    "        df_anomalies = df_anomalies.append(df_anomalies_temp, sort=False, ignore_index=True)\n",
    "    # Isolation forest returns negative score for anomalous records so converting them to positive.    \n",
    "    df_anomalies['Anomaly_Score'] = df_anomalies.Anomaly_Score*-1\n",
    "    #Get the feature importance\n",
    "    df_anomaly_x = df_anomalies.drop(['Currency_Pair','Year','Month','Day', 'Hour', 'Anomaly_Score'], axis=1)\n",
    "    df_x_output = pd.DataFrame(df_anomaly_x.values)\n",
    "    f_list = list(df_anomaly_x.columns)\n",
    "    df_feature_anomalies = pd.DataFrame([], columns=f_list)\n",
    "    df_Currency_Pair_list = list(df_anomalies.Currency_Pair.unique())\n",
    "    scaler = MinMaxScaler()\n",
    "    df_in_anomalies = pd.DataFrame([])\n",
    "    for Currency_Pair in df_Currency_Pair_list:\n",
    "        df_anomalies_original = pd.DataFrame(df_anomalies[df_anomalies.Currency_Pair == Currency_Pair])\n",
    "        df_x = df_anomalies_original.drop(['Currency_Pair','Year','Month','Day', 'Hour', 'Anomaly_Score'], axis=1)\n",
    "        df_x = df_x.astype(np.float64)\n",
    "        df_x_original = df_x.copy()\n",
    "        scaled_values = scaler.fit_transform(df_x)\n",
    "        df = pd.DataFrame(scaled_values, columns=f_list)\n",
    "        df = df.subtract(df.mean(), axis=1)\n",
    "        df = abs(df)\n",
    "        df_feature_anomalies = pd.concat([df_feature_anomalies, df], axis=0, ignore_index=True, sort=False)\n",
    "    df_feature_anomalies.columns = ['Score_' + col for col in df_feature_anomalies.columns]\n",
    "    return df_anomalies, df_feature_anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_and_format_data(df_anomalies, df_feature_anomalies, trade_timing):\n",
    "    df_feature_anomalies['Year'] = df_anomalies.Year\n",
    "    df_feature_anomalies['Month'] = df_anomalies.Month\n",
    "    df_feature_anomalies['Day'] = df_anomalies.Day\n",
    "    df_feature_anomalies['Hour'] = df_anomalies.Hour\n",
    "    df_feature_anomalies['Currency_Pair'] = df_anomalies.Currency_Pair\n",
    "    \n",
    "    df_top5_score = df_anomalies.groupby('Currency_Pair')['Anomaly_Score'].nlargest(5).sum(level=0).reset_index()\n",
    "    #df_top3_score = df_anomalies.groupby('interaction_from')['Anomaly_Score'].nlargest(3).sum(level=0).reset_index()\n",
    "    df_top1_score = df_anomalies.groupby('Currency_Pair')['Anomaly_Score'].nlargest(1).sum(level=0).reset_index()\n",
    "    # in case nlargest does not return n values, it will take the 1st largest.\n",
    "    if df_top5_score.columns[0] == 'index':\n",
    "        df_top_n_score = df_top1_score\n",
    "    else:\n",
    "        df_top_n_score = df_top5_score\n",
    "    df_top_n_score.columns = ['Currency_Pair','Anomaly_Score_Agg']\n",
    "    df_anomalies_final = df_anomalies.merge(df_top_n_score, how='left', on='Currency_Pair')\n",
    "    anomaly_score = df_anomalies_final[['Currency_Pair','Year','Month', 'Day', 'Hour','Anomaly_Score', 'Anomaly_Score_Agg']]\n",
    "    df_anomalies = df_anomalies.drop('Anomaly_Score',axis=1)\n",
    "    \n",
    "    #df_anomalies = df_anomalies.drop('Anomaly_Score',axis=1)\n",
    "    df_anomalies_pivot = df_anomalies.melt(id_vars=[\"Currency_Pair\",'Year','Month', \"Day\", 'Hour'], var_name=\"Features_for_Anomaly\", value_name=\"Feature_Stats\")\n",
    "    df_feature_anomalies_pivot = df_feature_anomalies.melt(id_vars=[\"Currency_Pair\",'Year','Month', \"Day\", 'Hour'], var_name=\"Reasons_for_Anomaly\", value_name=\"Feature_Score\")\n",
    "    df_anomalies_pivot = df_anomalies_pivot.set_index([\"Currency_Pair\",'Year','Month', \"Day\", 'Hour', df_anomalies_pivot.groupby([\"Currency_Pair\",'Year','Month', \"Day\", 'Hour']).cumcount()])\n",
    "    df_feature_anomalies_pivot = df_feature_anomalies_pivot.set_index([\"Currency_Pair\",'Year','Month', \"Day\", 'Hour', df_feature_anomalies_pivot.groupby([\"Currency_Pair\",'Year','Month', \"Day\", 'Hour']).cumcount()])\n",
    "    df_anomaly_feature_merged_pivot = (pd.concat([df_anomalies_pivot, df_feature_anomalies_pivot],axis=1)\n",
    "             .sort_index(level=2)\n",
    "             .reset_index(level=5, drop=True)\n",
    "             .reset_index())\n",
    "    feature_stats_median = df_anomaly_feature_merged_pivot.groupby(['Currency_Pair','Features_for_Anomaly']).Feature_Stats.median().reset_index()\n",
    "    feature_stats_median.columns = ['Currency_Pair','Features_for_Anomaly','Features_Stats_Meadian']\n",
    "    anomaly_stats_feature_score = df_anomaly_feature_merged_pivot.merge(feature_stats_median, on = ['Currency_Pair','Features_for_Anomaly'], how='left')\n",
    "    #anomaly_stats_feature_score = anomaly_stats_feature_score.assign(Algorithm='isolation-forest', duration_window = duration_window)\n",
    "    anomaly_stats_feature_score.drop('Reasons_for_Anomaly', axis=1, inplace=True)\n",
    "    anomaly_stats_feature_score = anomaly_stats_feature_score.round(3)\n",
    "    anomaly_score = anomaly_score.round(3)\n",
    "    anomaly_stats_feature_score['Trade_Timing'] = trade_timing\n",
    "    anomaly_score['Trade_Timing'] = trade_timing\n",
    "    return anomaly_score, anomaly_stats_feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n"
     ]
    }
   ],
   "source": [
    "df_anomalies, df_feature_anomalies = train(data_non_sensitive, 1.0)\n",
    "anomaly_score_1, anomaly_stats_feature_score_1 = pivot_and_format_data(df_anomalies, df_feature_anomalies, 'Normal Trading Time Periods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:466: RuntimeWarning: invalid value encountered in true_divide\n",
      "  * _average_path_length([self.max_samples_]))\n"
     ]
    }
   ],
   "source": [
    "df_anomalies, df_feature_anomalies = train(data_sensitive, 1.0)\n",
    "anomaly_score_2, anomaly_stats_feature_score_2 = pivot_and_format_data(df_anomalies, df_feature_anomalies, 'Price Sensitive Trading Time Periods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_stats_feature_score = pd.concat([anomaly_stats_feature_score_1, anomaly_stats_feature_score_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score = pd.concat([anomaly_score_1, anomaly_score_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_sensitive['Trade_Timing'] = 'Normal Trading Time Periods'\n",
    "data_sensitive['Trade_Timing'] = 'Price Sensitive Trading Time Periods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_non_sensitive, data_sensitive], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_stats_feature_score.to_csv(r'C:\\NICE Documents\\Bank of Indonesia\\anomaly_stats_feature_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score.to_csv(r'C:\\NICE Documents\\Bank of Indonesia\\anomaly_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.round(3).to_csv(r'C:\\NICE Documents\\Bank of Indonesia\\Feature_Stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Currency_Pair              object\n",
       "Year                        int64\n",
       "Month                       int64\n",
       "Day                         int64\n",
       "Hour                        int64\n",
       "Features_for_Anomaly       object\n",
       "Feature_Stats             float64\n",
       "Feature_Score             float64\n",
       "Features_Stats_Meadian    float64\n",
       "Trade_Timing               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_stats_feature_score.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
