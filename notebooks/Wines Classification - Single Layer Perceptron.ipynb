{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Amrendra\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#importing and downloading the data\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data\", one_hot = True) #one hot encoding for Y lables\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "file_path='D:\\\\My Personal Documents\\\\Learnings\\\\Data Science\\\\Data Sets\\\\Edureka Lab\\wine.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wine</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0     1    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1     1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2     1    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "3     1    14.37        1.95  2.50  16.8  113     3.85        3.49   \n",
       "4     1    13.24        2.59  2.87  21.0  118     2.80        2.69   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  \n",
       "3                  0.24     2.18       7.80  0.86  3.45     1480  \n",
       "4                  0.39     1.82       4.32  1.04  2.93      735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Wine\n",
    "x = data.drop('Wine', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test, y_train, y_test = train_test_split(x,y, test_size =0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 300 #number of interations\n",
    "batch_size = 100 #55000 images..\n",
    "display_step = 1\n",
    "cost_history =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network related\n",
    "\n",
    "\n",
    "n_input = 13\n",
    "n_hidden_1 = 200\n",
    "#n_hidden_2 = 200\n",
    "n_classes =3\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])  #[100,784] \n",
    "y = tf.placeholder(\"float\", [None, n_classes]) #[100,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelayer_perceptron(x,weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1']) #netl1 #x.h1+b1 [100,784].[784,256] = [100,256]\n",
    "    layer_1 = tf.nn.relu(layer_1) #activation #out l1\n",
    "   # layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) #layer_1*h2+b2 [100,256].[256,256] = [100,256]\n",
    "   # layer_2 = tf.nn.relu(layer_2) #activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out'])+biases['out'] #layer_2*out_w+b_out [100,256].[256,10] = [100,10]\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input,n_hidden_1])),   #[784, 256]\n",
    "   # 'h2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])), #[256, 256]\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes])) #[256, 10]\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),  #[256]\n",
    "   # 'b2': tf.Variable(tf.random_normal([n_hidden_2])),   #[256]\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))  #[10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1 = tf.layers.dense(x,3,activation = 'relu')\n",
    "#l1 = tf.layers.dense(l1, 3,activation = 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-1ac9f73fbb2b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = singlelayer_perceptron(x,weights,biases) #[100,10]\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels =y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 4695.105957031\n",
      "Epoch: 0002 cost= 3191.132080078\n",
      "Epoch: 0003 cost= 1687.741455078\n",
      "Epoch: 0004 cost= 1558.095825195\n",
      "Epoch: 0005 cost= 1275.210327148\n",
      "Epoch: 0006 cost= 1101.393432617\n",
      "Epoch: 0007 cost= 1522.340942383\n",
      "Epoch: 0008 cost= 1783.914550781\n",
      "Epoch: 0009 cost= 1822.193603516\n",
      "Epoch: 0010 cost= 1692.912353516\n",
      "Epoch: 0011 cost= 1432.994384766\n",
      "Epoch: 0012 cost= 1106.885986328\n",
      "Epoch: 0013 cost= 1057.311401367\n",
      "Epoch: 0014 cost= 1122.539550781\n",
      "Epoch: 0015 cost= 1007.975036621\n",
      "Epoch: 0016 cost= 712.105773926\n",
      "Epoch: 0017 cost= 368.953643799\n",
      "Epoch: 0018 cost= 267.888061523\n",
      "Epoch: 0019 cost= 564.331848145\n",
      "Epoch: 0020 cost= 620.869506836\n",
      "Epoch: 0021 cost= 483.036651611\n",
      "Epoch: 0022 cost= 555.075256348\n",
      "Epoch: 0023 cost= 506.478607178\n",
      "Epoch: 0024 cost= 374.988311768\n",
      "Epoch: 0025 cost= 339.366790771\n",
      "Epoch: 0026 cost= 169.233047485\n",
      "Epoch: 0027 cost= 33.357231140\n",
      "Epoch: 0028 cost= 301.660339355\n",
      "Epoch: 0029 cost= 370.856323242\n",
      "Epoch: 0030 cost= 224.292282104\n",
      "Epoch: 0031 cost= 222.952163696\n",
      "Epoch: 0032 cost= 290.003448486\n",
      "Epoch: 0033 cost= 241.982727051\n",
      "Epoch: 0034 cost= 125.255172729\n",
      "Epoch: 0035 cost= 125.094490051\n",
      "Epoch: 0036 cost= 169.124511719\n",
      "Epoch: 0037 cost= 97.893470764\n",
      "Epoch: 0038 cost= 95.169692993\n",
      "Epoch: 0039 cost= 18.979360580\n",
      "Epoch: 0040 cost= 56.491931915\n",
      "Epoch: 0041 cost= 99.489219666\n",
      "Epoch: 0042 cost= 92.375953674\n",
      "Epoch: 0043 cost= 49.559719086\n",
      "Epoch: 0044 cost= 6.660751820\n",
      "Epoch: 0045 cost= 27.556905746\n",
      "Epoch: 0046 cost= 41.721115112\n",
      "Epoch: 0047 cost= 53.242694855\n",
      "Epoch: 0048 cost= 22.721649170\n",
      "Epoch: 0049 cost= 13.810708046\n",
      "Epoch: 0050 cost= 20.656719208\n",
      "Epoch: 0051 cost= 30.417449951\n",
      "Epoch: 0052 cost= 28.722021103\n",
      "Epoch: 0053 cost= 26.095029831\n",
      "Epoch: 0054 cost= 22.044485092\n",
      "Epoch: 0055 cost= 7.408051968\n",
      "Epoch: 0056 cost= 16.668119431\n",
      "Epoch: 0057 cost= 27.830261230\n",
      "Epoch: 0058 cost= 16.970809937\n",
      "Epoch: 0059 cost= 20.949508667\n",
      "Epoch: 0060 cost= 17.767948151\n",
      "Epoch: 0061 cost= 8.110611916\n",
      "Epoch: 0062 cost= 13.485154152\n",
      "Epoch: 0063 cost= 16.500280380\n",
      "Epoch: 0064 cost= 13.088618279\n",
      "Epoch: 0065 cost= 12.312425613\n",
      "Epoch: 0066 cost= 12.224315643\n",
      "Epoch: 0067 cost= 6.615776539\n",
      "Epoch: 0068 cost= 10.522771835\n",
      "Epoch: 0069 cost= 11.190462112\n",
      "Epoch: 0070 cost= 8.010296822\n",
      "Epoch: 0071 cost= 12.068922043\n",
      "Epoch: 0072 cost= 8.654370308\n",
      "Epoch: 0073 cost= 8.834528923\n",
      "Epoch: 0074 cost= 10.837493896\n",
      "Epoch: 0075 cost= 8.317529678\n",
      "Epoch: 0076 cost= 6.553111553\n",
      "Epoch: 0077 cost= 9.350964546\n",
      "Epoch: 0078 cost= 6.214775562\n",
      "Epoch: 0079 cost= 7.394036770\n",
      "Epoch: 0080 cost= 7.586284637\n",
      "Epoch: 0081 cost= 5.351903439\n",
      "Epoch: 0082 cost= 8.476962090\n",
      "Epoch: 0083 cost= 6.129701138\n",
      "Epoch: 0084 cost= 6.687750816\n",
      "Epoch: 0085 cost= 8.054568291\n",
      "Epoch: 0086 cost= 5.826944351\n",
      "Epoch: 0087 cost= 6.270719051\n",
      "Epoch: 0088 cost= 5.868450642\n",
      "Epoch: 0089 cost= 5.433930874\n",
      "Epoch: 0090 cost= 6.294154167\n",
      "Epoch: 0091 cost= 4.849162579\n",
      "Epoch: 0092 cost= 5.359786987\n",
      "Epoch: 0093 cost= 5.119221210\n",
      "Epoch: 0094 cost= 4.463921547\n",
      "Epoch: 0095 cost= 5.368752956\n",
      "Epoch: 0096 cost= 4.558585167\n",
      "Epoch: 0097 cost= 4.871161938\n",
      "Epoch: 0098 cost= 4.893318653\n",
      "Epoch: 0099 cost= 4.301759720\n",
      "Epoch: 0100 cost= 5.162563801\n",
      "Epoch: 0101 cost= 4.607700825\n",
      "Epoch: 0102 cost= 4.610725880\n",
      "Epoch: 0103 cost= 5.149915218\n",
      "Epoch: 0104 cost= 4.198275089\n",
      "Epoch: 0105 cost= 4.956915379\n",
      "Epoch: 0106 cost= 4.512902737\n",
      "Epoch: 0107 cost= 4.430591106\n",
      "Epoch: 0108 cost= 5.009712219\n",
      "Epoch: 0109 cost= 4.077163219\n",
      "Epoch: 0110 cost= 4.584598541\n",
      "Epoch: 0111 cost= 4.470644474\n",
      "Epoch: 0112 cost= 4.029148579\n",
      "Epoch: 0113 cost= 4.322709560\n",
      "Epoch: 0114 cost= 4.011272907\n",
      "Epoch: 0115 cost= 3.760029078\n",
      "Epoch: 0116 cost= 4.179873466\n",
      "Epoch: 0117 cost= 3.684225559\n",
      "Epoch: 0118 cost= 3.826031208\n",
      "Epoch: 0119 cost= 3.692096472\n",
      "Epoch: 0120 cost= 3.616184711\n",
      "Epoch: 0121 cost= 3.864618778\n",
      "Epoch: 0122 cost= 3.523837566\n",
      "Epoch: 0123 cost= 3.533583879\n",
      "Epoch: 0124 cost= 3.464889526\n",
      "Epoch: 0125 cost= 3.393819332\n",
      "Epoch: 0126 cost= 3.582345963\n",
      "Epoch: 0127 cost= 3.264493942\n",
      "Epoch: 0128 cost= 3.329313278\n",
      "Epoch: 0129 cost= 3.234227657\n",
      "Epoch: 0130 cost= 3.203056335\n",
      "Epoch: 0131 cost= 3.215114355\n",
      "Epoch: 0132 cost= 3.106589317\n",
      "Epoch: 0133 cost= 3.082309246\n",
      "Epoch: 0134 cost= 3.097847462\n",
      "Epoch: 0135 cost= 3.022813082\n",
      "Epoch: 0136 cost= 2.965451479\n",
      "Epoch: 0137 cost= 2.948633194\n",
      "Epoch: 0138 cost= 2.929864407\n",
      "Epoch: 0139 cost= 2.874531031\n",
      "Epoch: 0140 cost= 2.829370737\n",
      "Epoch: 0141 cost= 2.829207659\n",
      "Epoch: 0142 cost= 2.762092829\n",
      "Epoch: 0143 cost= 2.739417076\n",
      "Epoch: 0144 cost= 2.713525057\n",
      "Epoch: 0145 cost= 2.679583549\n",
      "Epoch: 0146 cost= 2.640914440\n",
      "Epoch: 0147 cost= 2.613169432\n",
      "Epoch: 0148 cost= 2.570306063\n",
      "Epoch: 0149 cost= 2.537307978\n",
      "Epoch: 0150 cost= 2.515192509\n",
      "Epoch: 0151 cost= 2.480627775\n",
      "Epoch: 0152 cost= 2.439452648\n",
      "Epoch: 0153 cost= 2.422675848\n",
      "Epoch: 0154 cost= 2.386411190\n",
      "Epoch: 0155 cost= 2.367343664\n",
      "Epoch: 0156 cost= 2.331497908\n",
      "Epoch: 0157 cost= 2.308828831\n",
      "Epoch: 0158 cost= 2.282683849\n",
      "Epoch: 0159 cost= 2.253167152\n",
      "Epoch: 0160 cost= 2.226268291\n",
      "Epoch: 0161 cost= 2.205084801\n",
      "Epoch: 0162 cost= 2.184306145\n",
      "Epoch: 0163 cost= 2.160078764\n",
      "Epoch: 0164 cost= 2.135267258\n",
      "Epoch: 0165 cost= 2.119673491\n",
      "Epoch: 0166 cost= 2.091578484\n",
      "Epoch: 0167 cost= 2.073303699\n",
      "Epoch: 0168 cost= 2.052874565\n",
      "Epoch: 0169 cost= 2.029646635\n",
      "Epoch: 0170 cost= 2.009739876\n",
      "Epoch: 0171 cost= 1.987137914\n",
      "Epoch: 0172 cost= 1.965829372\n",
      "Epoch: 0173 cost= 1.945430040\n",
      "Epoch: 0174 cost= 1.923502326\n",
      "Epoch: 0175 cost= 1.903514981\n",
      "Epoch: 0176 cost= 1.881877184\n",
      "Epoch: 0177 cost= 1.860173345\n",
      "Epoch: 0178 cost= 1.839724422\n",
      "Epoch: 0179 cost= 1.818316340\n",
      "Epoch: 0180 cost= 1.798141837\n",
      "Epoch: 0181 cost= 1.776370049\n",
      "Epoch: 0182 cost= 1.755431771\n",
      "Epoch: 0183 cost= 1.734001875\n",
      "Epoch: 0184 cost= 1.712508082\n",
      "Epoch: 0185 cost= 1.691990733\n",
      "Epoch: 0186 cost= 1.670342565\n",
      "Epoch: 0187 cost= 1.649406433\n",
      "Epoch: 0188 cost= 1.627797246\n",
      "Epoch: 0189 cost= 1.606350183\n",
      "Epoch: 0190 cost= 1.585227728\n",
      "Epoch: 0191 cost= 1.563595891\n",
      "Epoch: 0192 cost= 1.542420268\n",
      "Epoch: 0193 cost= 1.520596027\n",
      "Epoch: 0194 cost= 1.499418139\n",
      "Epoch: 0195 cost= 1.477592587\n",
      "Epoch: 0196 cost= 1.456511140\n",
      "Epoch: 0197 cost= 1.434870958\n",
      "Epoch: 0198 cost= 1.413907647\n",
      "Epoch: 0199 cost= 1.393394351\n",
      "Epoch: 0200 cost= 1.373355985\n",
      "Epoch: 0201 cost= 1.357031226\n",
      "Epoch: 0202 cost= 1.354355454\n",
      "Epoch: 0203 cost= 1.366581559\n",
      "Epoch: 0204 cost= 1.320529580\n",
      "Epoch: 0205 cost= 1.393991232\n",
      "Epoch: 0206 cost= 1.277999163\n",
      "Epoch: 0207 cost= 1.440235376\n",
      "Epoch: 0208 cost= 1.301660776\n",
      "Epoch: 0209 cost= 1.346594095\n",
      "Epoch: 0210 cost= 1.297381163\n",
      "Epoch: 0211 cost= 1.315280080\n",
      "Epoch: 0212 cost= 1.347408533\n",
      "Epoch: 0213 cost= 1.170699835\n",
      "Epoch: 0214 cost= 1.307477951\n",
      "Epoch: 0215 cost= 1.225593567\n",
      "Epoch: 0216 cost= 1.306870341\n",
      "Epoch: 0217 cost= 1.113555551\n",
      "Epoch: 0218 cost= 1.183828712\n",
      "Epoch: 0219 cost= 1.158830762\n",
      "Epoch: 0220 cost= 1.153340697\n",
      "Epoch: 0221 cost= 1.082162738\n",
      "Epoch: 0222 cost= 1.057991624\n",
      "Epoch: 0223 cost= 1.092096329\n",
      "Epoch: 0224 cost= 1.088851213\n",
      "Epoch: 0225 cost= 1.015827537\n",
      "Epoch: 0226 cost= 1.028650999\n",
      "Epoch: 0227 cost= 1.015351534\n",
      "Epoch: 0228 cost= 1.019197226\n",
      "Epoch: 0229 cost= 0.984619439\n",
      "Epoch: 0230 cost= 0.965091646\n",
      "Epoch: 0231 cost= 0.951882720\n",
      "Epoch: 0232 cost= 0.966907203\n",
      "Epoch: 0233 cost= 0.946188569\n",
      "Epoch: 0234 cost= 0.930478811\n",
      "Epoch: 0235 cost= 0.896509290\n",
      "Epoch: 0236 cost= 0.903407991\n",
      "Epoch: 0237 cost= 0.886483610\n",
      "Epoch: 0238 cost= 0.861514747\n",
      "Epoch: 0239 cost= 0.862385988\n",
      "Epoch: 0240 cost= 0.857084453\n",
      "Epoch: 0241 cost= 0.827356219\n",
      "Epoch: 0242 cost= 0.828947008\n",
      "Epoch: 0243 cost= 0.823907733\n",
      "Epoch: 0244 cost= 0.808470130\n",
      "Epoch: 0245 cost= 0.830718279\n",
      "Epoch: 0246 cost= 0.849063694\n",
      "Epoch: 0247 cost= 0.842670977\n",
      "Epoch: 0248 cost= 0.796643496\n",
      "Epoch: 0249 cost= 0.753926098\n",
      "Epoch: 0250 cost= 0.772041023\n",
      "Epoch: 0251 cost= 0.760663629\n",
      "Epoch: 0252 cost= 0.732704997\n",
      "Epoch: 0253 cost= 0.728410900\n",
      "Epoch: 0254 cost= 0.726443648\n",
      "Epoch: 0255 cost= 0.706347167\n",
      "Epoch: 0256 cost= 0.704942167\n",
      "Epoch: 0257 cost= 0.697348237\n",
      "Epoch: 0258 cost= 0.684227586\n",
      "Epoch: 0259 cost= 0.680968642\n",
      "Epoch: 0260 cost= 0.670481682\n",
      "Epoch: 0261 cost= 0.657952487\n",
      "Epoch: 0262 cost= 0.651959419\n",
      "Epoch: 0263 cost= 0.644647658\n",
      "Epoch: 0264 cost= 0.636354864\n",
      "Epoch: 0265 cost= 0.627031326\n",
      "Epoch: 0266 cost= 0.619720638\n",
      "Epoch: 0267 cost= 0.613069236\n",
      "Epoch: 0268 cost= 0.604249775\n",
      "Epoch: 0269 cost= 0.596276760\n",
      "Epoch: 0270 cost= 0.589432478\n",
      "Epoch: 0271 cost= 0.580701113\n",
      "Epoch: 0272 cost= 0.571973324\n",
      "Epoch: 0273 cost= 0.565867007\n",
      "Epoch: 0274 cost= 0.557822883\n",
      "Epoch: 0275 cost= 0.548555315\n",
      "Epoch: 0276 cost= 0.541305065\n",
      "Epoch: 0277 cost= 0.533648789\n",
      "Epoch: 0278 cost= 0.525039852\n",
      "Epoch: 0279 cost= 0.517743707\n",
      "Epoch: 0280 cost= 0.510219693\n",
      "Epoch: 0281 cost= 0.501926482\n",
      "Epoch: 0282 cost= 0.493981630\n",
      "Epoch: 0283 cost= 0.486691862\n",
      "Epoch: 0284 cost= 0.479113609\n",
      "Epoch: 0285 cost= 0.471297890\n",
      "Epoch: 0286 cost= 0.464137375\n",
      "Epoch: 0287 cost= 0.457183421\n",
      "Epoch: 0288 cost= 0.450540870\n",
      "Epoch: 0289 cost= 0.444077045\n",
      "Epoch: 0290 cost= 0.435927868\n",
      "Epoch: 0291 cost= 0.428882271\n",
      "Epoch: 0292 cost= 0.424270391\n",
      "Epoch: 0293 cost= 0.423609883\n",
      "Epoch: 0294 cost= 0.424506247\n",
      "Epoch: 0295 cost= 0.411570728\n",
      "Epoch: 0296 cost= 0.409208119\n",
      "Epoch: 0297 cost= 0.427558929\n",
      "Epoch: 0298 cost= 0.415144473\n",
      "Epoch: 0299 cost= 0.404588193\n",
      "Epoch: 0300 cost= 0.386875838\n",
      "optimization finished\n",
      "Accuracy: 0.9859155\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs): #15\n",
    "        avg_cost =0.\n",
    "        #total_batch = int(data.num_examples/batch_size)  #55000/100 = 550 #550*100 = 55000\n",
    "        #for i in range(total_batch): #550\n",
    "          #  batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _,c = sess.run([optimizer,cost],feed_dict = {x:x_train,y:y_train})\n",
    "        avg_cost += c\n",
    "        if epoch % display_step ==0:\n",
    "            cost_history.append(avg_cost)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"optimization finished\")\n",
    "      #-------------------------------\n",
    "    correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x:x_train, y:y_train}))\n",
    "    plt.plot(cost_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
