{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and aggregation of email data for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "t = pd.Timestamp(\"today\").strftime(\"%Y-%m-%d\")\n",
    "filename = 'DataAggregationLogs_' + t + '.log'\n",
    "logging.basicConfig(filename=filename, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = json.loads(open('systemConfig').read())\n",
    "notebook_instance_name = config_file['notebook_instance_for_aggregation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('***********************************************')\n",
    "logging.info('Starting the Instance ' + notebook_instance_name)\n",
    "logging.info('***********************************************')\n",
    "logging.info('***********************************************')\n",
    "logging.info('Starting the Data Aggregation Script')\n",
    "logging.info('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_file_to_s3():\n",
    "    log_path = 's3://' + bucket + '/logs'\n",
    "    ! aws s3 cp $filename \"$log_path/$filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_notebook_instance():\n",
    "    ! aws sagemaker stop-notebook-instance --notebook-instance-name $notebook_instance_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    logging.info(str(pd.datetime.now()) + ': Importing libraries')\n",
    "    import os\n",
    "    import json\n",
    "    import s3fs\n",
    "    import boto3\n",
    "    from datetime import datetime\n",
    "    from io import StringIO\n",
    "    from cryptography.fernet import Fernet\n",
    "    import sagemaker as sage\n",
    "    from sagemaker import get_execution_role\n",
    "    logging.info(str(pd.datetime.now()) + ': Libraries imported sucessfully')\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + 'ERROR:' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a Secrets Manager client to retrieve the secret\n",
    "    logging.info(str(pd.datetime.now()) + ': Fetching configuration from secret manager')\n",
    "    sess = sage.Session()\n",
    "    region_name = sess.boto_session.region_name    \n",
    "    secret_name = config_file['secret_name']\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    config = json.loads(get_secret_value_response['SecretString'])\n",
    "    bucket = config['bucket']\n",
    "    encryption_key = config['encryption_key']\n",
    "    logging.info(str(pd.datetime.now()) + ': Configuration from secret manager fetched successully')    \n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + 'ERROR:' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    logging.info(str(pd.datetime.now()) + ': Setting up S3 path')\n",
    "    prefix = 's3://' + bucket + '/'\n",
    "    input_path = prefix+'input/data'\n",
    "    training_path = os.path.join(input_path, 'training')\n",
    "    testing_path = os.path.join(input_path, 'testing')\n",
    "    validation_path = os.path.join(input_path, 'validation')\n",
    "    processed_path = os.path.join(input_path, 'processed')\n",
    "    logging.info(str(pd.datetime.now()) + ': S3 path setup completed successfully')    \n",
    "except:  \n",
    "    logging.info(str(pd.datetime.now()) + 'ERROR:' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + ': Importing files from S3')\n",
    "    role = get_execution_role()\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    f_key = Fernet(encryption_key)\n",
    "    file_list = []\n",
    "    email_file_path = os.path.join(training_path, 'email')\n",
    "    try:\n",
    "        email_list = fs.ls(email_file_path)\n",
    "        email_file_list = []\n",
    "    #if len(email_list) > 0:\n",
    "        for email_file in email_list:\n",
    "            with fs.open('s3://{}'.format(email_file), 'rb') as f:\n",
    "                encrypted_data = f.read()\n",
    "            decrypted_data = f_key.decrypt(encrypted_data)\n",
    "            s = str(decrypted_data, 'utf-8')\n",
    "            data = StringIO(s)\n",
    "            df_email = pd.read_csv(data, encoding='utf-8')\n",
    "            email_file_list.append(df_email)\n",
    "        emails = pd.concat(email_file_list, sort=True)\n",
    "        file_list.append('email')\n",
    "    except:\n",
    "        logging.info(str(pd.datetime.now()) + ': Warning' + '- Email file not present')\n",
    "    chat_file_path = os.path.join(training_path, 'chat')\n",
    "    try:\n",
    "        chat_list = fs.ls(chat_file_path)\n",
    "        chat_file_list = []\n",
    "    #if len(email_list) > 0:\n",
    "        for chat_file in chat_list:\n",
    "            with fs.open('s3://{}'.format(chat_file), 'rb') as f:\n",
    "                encrypted_data = f.read()\n",
    "            decrypted_data = f_key.decrypt(encrypted_data)\n",
    "            s = str(decrypted_data, 'utf-8')\n",
    "            data = StringIO(s)\n",
    "            df_chat = pd.read_csv(data, encoding='utf-8')\n",
    "            chat_file_list.append(df_chat)\n",
    "        chats = pd.concat(chat_file_list, sort=True)\n",
    "        file_list.append('chat')\n",
    "    except:\n",
    "        logging.info(str(pd.datetime.now()) + ': Warning' + '- Chat file not present')\n",
    "    audio_file_path = os.path.join(training_path, 'voice')\n",
    "    try:\n",
    "        audio_list = fs.ls(audio_file_path)\n",
    "        audio_file_list = []\n",
    "    #if len(email_list) > 0:\n",
    "        for audio_file in audio_list:\n",
    "            with fs.open('s3://{}'.format(audio_file), 'rb') as f:\n",
    "                encrypted_data = f.read()\n",
    "            decrypted_data = f_key.decrypt(encrypted_data)\n",
    "            s = str(decrypted_data, 'utf-8')\n",
    "            data = StringIO(s)\n",
    "            df_audio = pd.read_csv(data, encoding='utf-8')\n",
    "            audio_file_list.append(df_audio)\n",
    "        audio = pd.concat(audio_file_list, sort=True)\n",
    "        file_list.append('voice')\n",
    "    except:\n",
    "        logging.info(str(pd.datetime.now()) + ': Warning'  + '- Voice file not present')\n",
    "    user_file_path = os.path.join(training_path, 'user')\n",
    "    try:\n",
    "        user_list = fs.ls(user_file_path)\n",
    "        user_file_list = []\n",
    "    #if len(email_list) > 0:\n",
    "        for user_file in user_list:\n",
    "            with fs.open('s3://{}'.format(user_file), 'rb') as f:\n",
    "                encrypted_data = f.read()\n",
    "            decrypted_data = f_key.decrypt(encrypted_data)\n",
    "            s = str(decrypted_data, 'utf-8')\n",
    "            data = StringIO(s)\n",
    "            df_user = pd.read_csv(data, encoding='utf-8')\n",
    "            user_file_list.append(df_user)\n",
    "        users = pd.concat(user_file_list, sort=True)\n",
    "        file_list.append('user')\n",
    "    except:\n",
    "        logging.info(str(pd.datetime.now()) + ': Warning' + '- User file not present')\n",
    "    logging.info(str(pd.datetime.now()) + ': Importing files from S3 Completed')\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(str(pd.datetime.now()) + ': Checking for files imported')\n",
    "if len(file_list) < 2:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- Source data files are not present. Stopping further progress!')\n",
    "    shutdown_notebook_instance()\n",
    "else:\n",
    "    logging.info(str(pd.datetime.now()) + ': Files importing check is complete.Ready to proceed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    logging.info(str(pd.datetime.now()) + ': Verifying the columns for the dataset imported')\n",
    "    #Column Verification for the dataset imported\n",
    "    if 'email' in file_list:\n",
    "        email_col_list = ['interaction_identifier', 'interaction_start', 'interaction_from',\n",
    "                    'interaction_to', 'interaction_cc', 'interaction_source',\n",
    "                    'interaction_type', 'interaction_has_attachment', 'childcount',\n",
    "                    'interaction_language', 'scope_BF', 'participant_count_BF_I', 'size', 'Party_ID_BF', 'interaction_original_author_id']\n",
    "    if 'chat' in file_list:\n",
    "        chat_col_list = ['interaction_identifier', 'interaction_start', 'interaction_from',\n",
    "                  'interaction_to', 'interaction_original_author_id',\n",
    "                  'interaction_language', 'interaction_type', 'interaction_source',\n",
    "                  'size', 'participant_count_BF_I']\n",
    "    if 'voice' in file_list:\n",
    "        audio_col_list = ['interaction_identifier', 'interaction_sourceid', 'interaction_type',\n",
    "                    'interaction_start', 'interaction_from', 'interaction_to',\n",
    "                    'interaction_direction', 'interaction_duration_int',\n",
    "                    'participant_count_BF_I', 'interaction_language', 'interaction_source',\n",
    "                    'interaction_author', 'interaction_original_author_id']\n",
    "    if 'user' in file_list:\n",
    "        user_col_list = ['multisource.username', 'multisource.firstname', 'multisource.lastname',\n",
    "                   'FullName_CUP', 'Region_CUP', 'Country_CUP', 'Title_CUP', 'ProductGroup_CUP',\n",
    "                   'Exchange_EUSID', 'Bloomberg_EUSID', 'Yieldbroker_EUSID',\n",
    "                   'NiceTradingRecording_EUSID',\n",
    "                   'Reuters_EUSID', 'NIM_EUSID', 'NTRExport_EUSID', 'Avaya_EUSID', 'Verint_EUSID', 'Intercall_EUSID']      \n",
    "    logging.info(str(pd.datetime.now()) + ': Column Verification for the dataset is completed')\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def handle(dataset, col_list, dataset_name):\n",
    "        col_not_present = [col for col in col_list if col not in dataset.columns]\n",
    "        if len(col_not_present) > 0:\n",
    "            logging.info(str(pd.datetime.now()) + \"Do not proceed further as \" + dataset_name + \" has some missing columns\")\n",
    "            for col1 in col_not_present:\n",
    "                logging.info(str(pd.datetime.now()) + dataset_name + \" does not have \" + col1 +\" column in the input file.\")\n",
    "            logging.info(str(pd.datetime.now()) + \"Stopping further execution\")\n",
    "            shutdown_notebook_instance()\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    logging.info(str(pd.datetime.now()) + \": Verifying columns in the dataset provided\")\n",
    "    if 'email' in file_list:\n",
    "        handle(emails, email_col_list, 'email')\n",
    "    logging.info(str(pd.datetime.now()) + \": Email dataset Verification is complete\")\n",
    "    if 'chat' in file_list:\n",
    "        handle(chats, chat_col_list, 'chat')\n",
    "    logging.info(str(pd.datetime.now()) + \": Chat dataset Verification is complete\")\n",
    "    if 'voice' in file_list:\n",
    "        handle(audio, audio_col_list, 'voice')\n",
    "    logging.info(str(pd.datetime.now()) + \": Audio dataset Verification is complete\")\n",
    "    if 'user' in file_list:\n",
    "        handle(users, user_col_list, 'user')\n",
    "    logging.info(str(pd.datetime.now()) + \": User dataset Verification is complete\")     \n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Limiting the source systems for communication\")\n",
    "    # Row Count dataset Validation\n",
    "    if 'email' in file_list:\n",
    "        email_sources = ['Exchange', 'Bloomberg']\n",
    "        email_records = emails[emails.interaction_source.isin(email_sources)].shape[0]\n",
    "        if email_records == 0:\n",
    "            file_list.remove('email')\n",
    "    if 'chat' in file_list:\n",
    "        chat_sources = ['Bloomberg', 'Yieldbroker', 'Reuters']\n",
    "        chat_records = chats[chats.interaction_source.isin(chat_sources)].shape[0]\n",
    "        if chat_records == 0:\n",
    "            file_list.remove('chat')\n",
    "    if 'voice' in file_list:\n",
    "        audio_sources = ['NiceTradingRecording', 'NTRExport', 'NIM', 'Avaya', 'Verint', 'InterCall']\n",
    "        audio_records = audio[audio.interaction_source.isin(audio_sources)].shape[0]\n",
    "        if audio_records == 0:\n",
    "            file_list.remove('voice')\n",
    "    if 'user' in file_list:\n",
    "        if users.shape[0] == 0:\n",
    "            file_list.remove('user')\n",
    "    logging.info(str(pd.datetime.now()) + \": Source systems configured\")\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Converting the datatype\")\n",
    "    #Convert user attributes column type to objest\n",
    "    if 'user' in file_list:\n",
    "        data_list = [users]\n",
    "        for dataset in data_list:\n",
    "            for cols in dataset.columns:\n",
    "                dataset[cols] = dataset[cols].astype('object')\n",
    "    if 'email' in file_list:\n",
    "        data_list = [emails]\n",
    "        for dataset in data_list:\n",
    "            for cols in dataset.columns:\n",
    "                dataset[cols] = dataset[cols].astype('object')  \n",
    "    logging.info(str(pd.datetime.now()) + \": Datatype conversion Complete\")\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for cols in [col for col in users.columns if col.endswith(('EUSID'))]:\n",
    "        users[cols] = users[cols].astype('str').apply(lambda x: x.lower())\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy_columns(df):\n",
    "    return [col for col in df.columns if col.startswith(\"interaction_risk_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email data processing( filtering, cleanup, missing value treatment, feature engineering and aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    email_policy_cols = extract_policy_columns(emails)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    def email_data_cleanup(df_email):\n",
    "        df_email_clean = df_email[df_email.interaction_type == 'Email']\n",
    "        df_email_clean = df_email_clean[df_email_clean.interaction_from.notnull()]\n",
    "        df_email_clean = df_email_clean[df_email_clean.interaction_start.notnull()]\n",
    "        df_email_clean = df_email_clean[df_email_clean.participant_count_BF_I.notnull()]\n",
    "        df_email_clean['size'] = df_email_clean['size'].fillna(0)\n",
    "        df_email_clean['participant_count_BF_I'] = df_email_clean['participant_count_BF_I'].fillna(0)\n",
    "        df_email_clean['childcount'] = df_email_clean['childcount'].fillna(0)\n",
    "        df_email_clean['size'] = df_email_clean['size'].astype('int')\n",
    "        df_email_clean['participant_count_BF_I'] = df_email_clean['participant_count_BF_I'].astype('int')\n",
    "        df_email_clean['childcount'] = df_email_clean['childcount'].astype('int')\n",
    "        for col in email_policy_cols:\n",
    "            df_email_clean[col] = df_email_clean[col].fillna(0)\n",
    "        #df_email_clean=pd.concat(df_email_clean_exchange,df_email_clean_exchange)\n",
    "        return df_email_clean\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def email_data_feature_generation(df_email_clean, bloomberg_user_mapping):\n",
    "        #Derive new features from existing ones\n",
    "        df_email_clean = df_email_clean[df_email_clean.interaction_start.str.contains('T') == True]\n",
    "        df_email_clean['Date'] = df_email_clean.interaction_start.map(lambda x: x.split('T')[0].strip())\n",
    "        df_email_clean['Hour'] = df_email_clean.interaction_start.map(lambda x: int(x.split('T')[1].strip().split(':')[0]))\n",
    "        df_email_clean['Minute'] = df_email_clean.interaction_start.map(lambda x: int(x.split('T')[1].strip().split(':')[1]))\n",
    "        df_email_clean['Cal_Minutes'] = df_email_clean.Hour*60 + df_email_clean.Minute\n",
    "        df_email_clean['Content_Language'] = df_email_clean.interaction_language.map(lambda x: 0 if x == 'English' else 1 if pd.notnull(x) else 0)\n",
    "        df_email_clean_exchange = df_email_clean[df_email_clean.interaction_source == 'Exchange']\n",
    "        if df_email_clean_exchange.shape[0]!=0:\n",
    "            df_email_clean_exchange = df_email_clean_exchange[df_email_clean_exchange.interaction_from.str.contains('@') == True]\n",
    "            df_email_clean_exchange.interaction_from = df_email_clean_exchange.interaction_from.map(lambda x: x if x.find('<') | x.find('>') == -1 else x[x.find('<')+1:x.find('>')])\n",
    "            df_email_clean_exchange.interaction_to = df_email_clean_exchange.apply(lambda x: \"|\".join(list(set(list(x.interaction_original_author_id.replace(x.interaction_from, '').split('|'))))), axis=1)\n",
    "        df_email_clean_bloomberg = df_email_clean[df_email_clean.interaction_source=='Bloomberg']\n",
    "        if df_email_clean_bloomberg.shape[0]!=0:\n",
    "            df_email_clean_bloomberg = df_email_clean_bloomberg[df_email_clean_bloomberg.Party_ID_BF.str.contains('\\|') == True]\n",
    "            df_email_clean_bloomberg.interaction_from = df_email_clean_bloomberg.Party_ID_BF.map(lambda x: x.split('|')[1].strip())\n",
    "            df_email_clean_bloomberg.interaction_to = df_email_clean_bloomberg.apply(lambda x: \"|\".join(list(set(list(x.interaction_original_author_id.split('|'))[x.participant_count_BF_I +1:]))), axis=1)\n",
    "        df_email_clean = pd.concat([df_email_clean_exchange, df_email_clean_bloomberg])\n",
    "        df_email_clean = df_email_clean.merge(bloomberg_user_mapping, how='left', left_on='interaction_from', right_on='Bloomberg_EUSID')\n",
    "        df_email_clean.interaction_from = df_email_clean.Exchange_EUSID.fillna(df_email_clean.interaction_from)\n",
    "        df_email_clean.drop(['Bloomberg_EUSID','Exchange_EUSID'], axis=1, inplace=True)\n",
    "        df = pd.DataFrame(list(zip(df_email_clean.Date, df_email_clean.interaction_to, df_email_clean.Cal_Minutes)))\n",
    "        # Create a list of emails in To and CC with sent date\n",
    "        date_list = df[0].unique()\n",
    "        To_List_Expended=[]\n",
    "        for date in date_list:\n",
    "            for date1, email, minutes in zip(df[0].values, df[1].values, df[2].values):\n",
    "                if date==date1:\n",
    "                    l1 = email.split('|')\n",
    "                    for l2 in l1:\n",
    "                        To_List_Expended.append(l2.strip()+ \"~\" + date1 + \"~\" + str(minutes))\n",
    "        #Split Email, Sent Date and Email received minutes as seperate column\n",
    "        Rec_Email = []\n",
    "        Sent_Date = []\n",
    "        Rec_Minutes = []\n",
    "        for i in To_List_Expended:\n",
    "            l = i.split('~')\n",
    "            if l[0]:\n",
    "                Rec_Email.append(l[0])\n",
    "                Sent_Date.append(l[1])\n",
    "                Rec_Minutes.append(int(l[2]))   \n",
    "        #Aggregate features at day level    \n",
    "        df_temp = pd.DataFrame(list(zip(Rec_Email, Sent_Date, Rec_Minutes)))\n",
    "        if df_temp.shape[0] == 0:\n",
    "            df_temp = pd.DataFrame(columns=['Rec_Email','Date', 'Mean_Email_Rec_Time'])\n",
    "        else:\n",
    "            df_temp=df_temp.rename(index=str, columns={0: \"Rec_Email\", 1: \"Date\", 2: \"Mean_Email_Rec_Time\"})\n",
    "        df_temp = df_temp[df_temp.Rec_Email.notnull()]\n",
    "        df_temp= df_temp.merge(bloomberg_user_mapping, how='left', left_on='Rec_Email', right_on='Bloomberg_EUSID')\n",
    "        df_temp.Rec_Email = df_temp.Exchange_EUSID.fillna(df_temp.Rec_Email)\n",
    "        df_temp.drop(['Bloomberg_EUSID','Exchange_EUSID'], axis=1, inplace=True)\n",
    "        df = df_temp.groupby(['Rec_Email', 'Date']).size().reset_index()\n",
    "        df1_sum_rec_time = df_temp.groupby(['Rec_Email', 'Date']).Mean_Email_Rec_Time.sum().reset_index(name=\"Sum_Email_Rec_Time\")\n",
    "        df1_count_rec_time = df_temp.groupby(['Rec_Email', 'Date']).Mean_Email_Rec_Time.count().reset_index(name=\"Count_Email_Rec_Time\")\n",
    "        df1=pd.merge(df1_sum_rec_time, df1_count_rec_time, how='inner', on=['Rec_Email', 'Date'])\n",
    "        df_email_From_grp = df_email_clean.groupby(['interaction_from', 'Date'])\n",
    "        df_email_stats = df_email_From_grp.interaction_start.count().reset_index()\n",
    "        df_email_stats = df_email_stats.rename(index=str, columns={\"interaction_start\": \"Email_Sent\"})\n",
    "        df_email_stats['Sum_Email_Recipient'] = df_email_From_grp.participant_count_BF_I.sum().reset_index().participant_count_BF_I.values\n",
    "        df_email_stats['Count_Email_Recipient'] = df_email_From_grp.participant_count_BF_I.count().reset_index().participant_count_BF_I.values\n",
    "        df_email_stats['Max_NonEnglish_Emails'] = df_email_From_grp.Content_Language.sum().reset_index().Content_Language.values\n",
    "        df_email_stats['Sum_Email_Sent_Time'] = df_email_From_grp.Cal_Minutes.sum().reset_index().Cal_Minutes.values\n",
    "        df_email_stats['Count_Email_Sent_Time'] = df_email_From_grp.Cal_Minutes.count().reset_index().Cal_Minutes.values\n",
    "        df_email_stats['Sum_Email_Size'] = df_email_From_grp['size'].sum().values\n",
    "        df_email_stats['Count_Email_Size'] = df_email_From_grp['size'].count().values\n",
    "        df_email_stats['Emails_with_Attachment'] = df_email_From_grp.interaction_has_attachment.count().reset_index().interaction_has_attachment.values\n",
    "        df_email_stats['Total_Attachment_in_Emails'] = df_email_From_grp.childcount.sum().reset_index().childcount.values        \n",
    "        for col in email_policy_cols:\n",
    "            df_email_stats['email_' + col] = df_email_From_grp[col].mean().values\n",
    "        df_email_stats = df_email_stats.merge(df, how='outer', left_on=['interaction_from', 'Date'], right_on=['Rec_Email','Date'])\n",
    "        df_email_stats = df_email_stats.rename(columns={0: 'Email_Received'})\n",
    "        df_email_stats.Email_Received.fillna(0, inplace=True)\n",
    "        df_email_stats.interaction_from = df_email_stats.interaction_from.fillna(df_email_stats.Rec_Email)\n",
    "        df_email_stats.Date = df_email_stats.Date.fillna(df_email_stats.Date)\n",
    "        df_email_stats.Email_Sent.fillna(0, inplace=True)\n",
    "        df_email_stats.drop(['Rec_Email'], axis=1, inplace=True)\n",
    "        df_email_stats=df_email_stats.merge(df1, how='left',left_on=['interaction_from','Date'], right_on=['Rec_Email', 'Date']).drop('Rec_Email', axis=1)\n",
    "        df_email_stats.Sum_Email_Rec_Time.fillna(df_email_stats.Sum_Email_Sent_Time, inplace=True)\n",
    "        df_email_stats.dropna(subset=['interaction_from'], inplace=True)\n",
    "        df_email_stats.fillna(0, inplace=True)\n",
    "        return df_email_stats    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def email_cleanup_aggregation(df_emails,users):\n",
    "        cols = ['interaction_identifier',\n",
    "                'interaction_start',\n",
    "                'interaction_from',\n",
    "                'interaction_to',\n",
    "                'interaction_cc',\n",
    "                'interaction_type',\n",
    "                'interaction_source',\n",
    "                'interaction_has_attachment',\n",
    "                'childcount',\n",
    "                'interaction_language',\n",
    "                'scope_BF',\n",
    "                'participant_count_BF_I',\n",
    "                'size',\n",
    "                'Party_ID_BF',\n",
    "                'interaction_original_author_id'\n",
    "                ]\n",
    "        df_emails = df_emails[cols + email_policy_cols]        \n",
    "        col_list = ['interaction_from', 'interaction_to', 'interaction_cc', 'Party_ID_BF', 'interaction_original_author_id']        \n",
    "        for col in col_list:\n",
    "            df_emails[col] = df_emails[col].astype('str').apply(lambda x: x.lower())\n",
    "        bloomberg_user_mapping = pd.DataFrame(list(zip(users.Bloomberg_EUSID, users.Exchange_EUSID)), columns=['Bloomberg_EUSID', 'Exchange_EUSID'])\n",
    "        bloomberg_user_mapping.dropna(inplace=True)\n",
    "        df_email_clean = email_data_cleanup(df_emails)\n",
    "        df_email_stats =email_data_feature_generation(df_email_clean,bloomberg_user_mapping)\n",
    "        return df_email_stats    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Executing Email cleaning and aggregation\")\n",
    "#Executing the cleaning and the aggregation script\n",
    "    if 'email' in file_list and 'user' in file_list:\n",
    "        df_email_stats = email_cleanup_aggregation(emails,users)\n",
    "        logging.info(str(pd.datetime.now()) + \": Execution of Email cleaning and Aggregation is complete\")\n",
    "                     \n",
    "    else:\n",
    "        df_email_stats = pd.DataFrame(columns=['interaction_from', 'Date', 'Email_Sent', 'Sum_Email_Recipient','Count_Email_Recipient',\n",
    "                                               'Max_NonEnglish_Emails', 'Sum_Email_Sent_Time','Count_Email_Sent_Time',\n",
    "                                               'Sum_Email_Size','Count_Email_Size',\n",
    "                                               'Emails_with_Attachment', 'Total_Attachment_in_Emails',\n",
    "                                               'Email_Received', 'Sum_Email_Rec_Time', 'Count_Email_Rec_Time'])\n",
    "        logging.info(str(pd.datetime.now()) + \": Email Cleaning and aggregation will not be processed as either Email or User file is missing or there are no rows in it\")    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat data processing( filtering, cleanup, missing value treatment, feature engineering and aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chat_policy_cols = extract_policy_columns(chats)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def chat_dataset_cleanup(dataset):\n",
    "        columns = list(dataset.columns)\n",
    "        interaction_sources = ['Bloomberg', 'Yieldbroker', 'Reuters']    \n",
    "        dataset = dataset[dataset.interaction_source.isin(interaction_sources)]\n",
    "        for col in columns:\n",
    "            if dataset[col].isnull().any()==True:\n",
    "                if col in ('interaction_start', 'interaction_from', 'interaction_original_author_id'):\n",
    "                    dataset = dataset.dropna(axis=0, subset=[col])\n",
    "                if col in ('interaction_language'):\n",
    "                    dataset[col]  = dataset.groupby(\"interaction_from\")[col].transform(lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else \"UNK\")\n",
    "                if col in ('interaction_type','interaction_source'):\n",
    "                    dataset[col] = dataset[col].fillna('UNK')\n",
    "                else:    \n",
    "                    dataset[col] = dataset[col].fillna(0)\n",
    "        #dataset['chat_interaction_from']=dataset.interaction_original_author_id.map(lambda x: x.split('|')[0].strip())\n",
    "        dataset = dataset[dataset.interaction_start.str.contains('T') == True]\n",
    "        dataset['chat_interaction_start']=dataset.interaction_start.map(lambda x: x.split('|')[0].strip())\n",
    "        dataset['chat_date'] = dataset.chat_interaction_start.map(lambda x: x.split('T')[0].strip())\n",
    "        dataset['chat_hour'] = dataset.interaction_start.map(lambda x: int(x.split('T')[1].strip().split(':')[0]))\n",
    "        dataset['chat_minutes'] = dataset.interaction_start.map(lambda x: int(x.split('T')[1].strip().split(':')[1]))\n",
    "        dataset['chat_time'] = dataset.chat_hour*60 + dataset.chat_minutes\n",
    "        dataset['chat_non_english_flag'] = dataset.interaction_language.map(lambda x: 0 if x == 'English' else 1 if pd.notnull(x) else 0)\n",
    "        for col in chat_policy_cols:\n",
    "            dataset[col] = dataset[col].fillna(0)\n",
    "        return dataset    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "\n",
    "    def chat_data_preprocessing(chat_df):\n",
    "        # Create a list of emails in To and CC with sent date\n",
    "        identifier_list = chat_df['interaction_identifier'].unique()\n",
    "        identifier_list_expended = []\n",
    "        for identifier in identifier_list:\n",
    "            for identifier1,chatid in zip(chat_df['interaction_identifier'].values,chat_df['interaction_original_author_id'].values):\n",
    "                if identifier == identifier1:\n",
    "                    l1 = chatid.split('|')\n",
    "                    for l2 in l1:\n",
    "                        identifier_list_expended.append(l2.strip()+ \"~\" + identifier1)\n",
    "\n",
    "        chat_identifier=[]\n",
    "        chat_id=[]\n",
    "        for i in identifier_list_expended:\n",
    "            l=i.split('~')\n",
    "            chat_identifier.append(l[1])\n",
    "            chat_id.append(l[0])      \n",
    "\n",
    "        df_temp = pd.DataFrame(list(zip(chat_identifier, chat_id)))\n",
    "        df_temp.columns = ['interaction_identifier', 'chat_id']\n",
    "        df = df_temp.merge(chat_df, how='left', on='interaction_identifier')      \n",
    "\n",
    "        bloomberg_df = df[df.interaction_source == 'Bloomberg']\n",
    "        bloomberg_df=bloomberg_df[bloomberg_df['chat_id'].str.contains('@bloomberg.')]\n",
    "        bloomberg_users = users[['Exchange_EUSID', 'Bloomberg_EUSID']]\n",
    "        bloomberg_df = bloomberg_df.merge(bloomberg_users, how='left', left_on='chat_id', right_on='Bloomberg_EUSID')\n",
    "        bloomberg_df.drop('Bloomberg_EUSID', axis=1, inplace=True)\n",
    "\n",
    "        #fxt_df = df[df.interaction_source=='FXT']\n",
    "        #fxt_users = users[['Exchange_EUSID','FXT_EUSID']]\n",
    "        #fxt_df = fxt_df.merge(fxt_users, how='left', left_on='chat_id', right_on='FXT_EUSID')\n",
    "        #fxt_df.drop('FXT_EUSID',axis=1,inplace=True)\n",
    "\n",
    "        yieldbroker_df = df[df.interaction_source == 'Yieldbroker']\n",
    "        yieldbroker_users = users[['Exchange_EUSID', 'Yieldbroker_EUSID']]\n",
    "        yieldbroker_df = yieldbroker_df.merge(yieldbroker_users, how='left', left_on='chat_id', right_on='Yieldbroker_EUSID')\n",
    "        yieldbroker_df.drop('Yieldbroker_EUSID', axis=1, inplace=True)\n",
    "\n",
    "        thomsonreuters_df = df[df.interaction_source == 'Reuters']\n",
    "        thomsonreuters_users = users[['Exchange_EUSID', 'Reuters_EUSID']]\n",
    "        thomsonreuters_df = thomsonreuters_df.merge(thomsonreuters_users, how='left', left_on='chat_id', right_on='Reuters_EUSID')\n",
    "        thomsonreuters_df.drop('Reuters_EUSID', axis=1, inplace=True)\n",
    "\n",
    "        chat_df_processed = pd.concat([bloomberg_df, yieldbroker_df, thomsonreuters_df], axis=0)\n",
    "        chat_df_processed.Exchange_EUSID = chat_df_processed.Exchange_EUSID.fillna(chat_df_processed.chat_id)\n",
    "        chat_df_processed.drop(['chat_id'], axis=1, inplace=True)\n",
    "        chat_df_processed.rename(columns={'Exchange_EUSID':'chat_id'}, inplace=True)\n",
    "\n",
    "        return chat_df_processed\n",
    "    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    class chat_feature_generation:   \n",
    "        def window_selection(dataset,window):\n",
    "            if window=='daily':\n",
    "                duration_feature='chat_date'\n",
    "            if window=='weekly':\n",
    "                duration_feature='week_of_year'\n",
    "            if window=='monthly':\n",
    "                duration_feature='month_of_year'\n",
    "            if window=='quarter':\n",
    "                duration_feature='quarter_of_year'\n",
    "            grouped_data=dataset.groupby(['chat_id', duration_feature])   \n",
    "            def chat_sent(dataset):\n",
    "                file=dataset.interaction_identifier.nunique().reset_index(name='counts')\n",
    "                chat_sent=file.rename(columns={'counts': 'Chat_Count'})\n",
    "                return chat_sent \n",
    "            def sent_size_sum_user(dataset):\n",
    "                sum_chat_size=dataset['size'].sum().reset_index(name='Sum_Chat_Size')\n",
    "                return sum_chat_size\n",
    "            def sent_size_count_user(dataset):\n",
    "                count_chat_size=dataset['size'].count().reset_index(name='Count_Chat_Size')\n",
    "                return count_chat_size\n",
    "            def sum_recipient_per_user(dataset):\n",
    "                Sum_chat_Recipient=dataset['participant_count_BF_I'].sum().reset_index(name=\"Sum_chat_recipient\")\n",
    "                return Sum_chat_Recipient\n",
    "            def count_recipient_per_user(dataset):\n",
    "                Count_chat_Recipient=dataset['participant_count_BF_I'].count().reset_index(name=\"Count_chat_recipient\")\n",
    "                return Count_chat_Recipient\n",
    "            def Sum_time_for_chat(dataset):\n",
    "                Sum_chat_time=dataset['chat_time'].sum().reset_index(name=\"Sum_chat_time\")\n",
    "                return Sum_chat_time\n",
    "            def Count_time_for_chat(dataset):\n",
    "                Count_time_chat=dataset['chat_time'].count().reset_index(name=\"Count_chat_time\")\n",
    "                return Count_time_chat\n",
    "            def non_english_chat_detail(dataset):\n",
    "                non_eng_chat_detail=dataset.chat_non_english_flag.sum().reset_index(name=\"nonEnglish_chat_count\")\n",
    "                #non_eng_chat_detail=non_eng_chat_detail.rename(columns={'chat_non_english_flag': 'nonEnglish_chat_count'})\n",
    "                for col in chat_policy_cols:\n",
    "                    non_eng_chat_detail['chat_' + col]=dataset[col].mean().values\n",
    "                return non_eng_chat_detail\n",
    "            chat_sent=chat_sent(grouped_data)\n",
    "            sent_size_sum_user=sent_size_sum_user(grouped_data)\n",
    "            sent_size_count_user=sent_size_count_user(grouped_data)\n",
    "            sum_recipient_per_user=sum_recipient_per_user(grouped_data)\n",
    "            count_recipient_per_user=count_recipient_per_user(grouped_data)\n",
    "            Sum_time_for_chat=Sum_time_for_chat(grouped_data)\n",
    "            Count_time_for_chat=Count_time_for_chat(grouped_data)\n",
    "            non_english_chat=non_english_chat_detail(grouped_data)\n",
    "            s1 = pd.merge(chat_sent, sent_size_sum_user, how='inner', on=['chat_id',duration_feature])\n",
    "            dataframe_list=[sent_size_count_user,\n",
    "            sum_recipient_per_user,\n",
    "            count_recipient_per_user,\n",
    "            Sum_time_for_chat,\n",
    "            Count_time_for_chat,\n",
    "            non_english_chat]\n",
    "            for i in dataframe_list:\n",
    "                s1=pd.merge(s1, i, how='inner', on=['chat_id',duration_feature])\n",
    "            return s1        \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    def chat_email_aggregation(chats):\n",
    "        cols = ['interaction_identifier',\n",
    "        'interaction_start',\n",
    "        'interaction_from',\n",
    "        'interaction_to',\n",
    "        'interaction_original_author_id',\n",
    "        'interaction_language',\n",
    "        'interaction_type',\n",
    "        'interaction_source',\n",
    "        'size',\n",
    "        'participant_count_BF_I'\n",
    "        ]\n",
    "        #chats = chats[cols]        \n",
    "        col_list = ['interaction_from', 'interaction_to', 'interaction_original_author_id']        \n",
    "        for col in col_list:\n",
    "            chats[col] = chats[col].astype('str').apply(lambda x: x.lower())\n",
    "        chat_df=chat_dataset_cleanup(chats)\n",
    "        chat_cols = ['chat_date', 'chat_time', 'chat_non_english_flag', 'participant_count_BF_I', 'size', 'interaction_source', 'interaction_type', 'interaction_language', 'interaction_identifier', 'interaction_original_author_id'] + chat_policy_cols\n",
    "        chat_df=chat_df[chat_cols]\n",
    "        chat_df_processed = chat_data_preprocessing(chat_df)\n",
    "        df_chat_stats=chat_feature_generation.window_selection(chat_df_processed,'daily')\n",
    "        return df_chat_stats    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Executing Chat cleaning and aggregation\")\n",
    "    #Executing the cleaning and the aggregation script\n",
    "    if 'chat' in file_list and 'user' in file_list:\n",
    "        df_chat_stats = chat_email_aggregation(chats)\n",
    "        logging.info(str(pd.datetime.now()) + \": Execution of Chat cleaning and aggregation is complete\")\n",
    "    else:\n",
    "        df_chat_stats = pd.DataFrame(columns=['chat_id', 'chat_date', 'Chat_Count', 'Sum_Chat_Size',\n",
    "                                               'Count_Chat_Size', 'Sum_chat_recipient', 'Count_chat_recipient',\n",
    "                                               'Sum_chat_time', 'Count_chat_time', 'nonEnglish_chat_count'])\n",
    "        logging.info(str(pd.datetime.now()) + \": Chat Cleaning and aggregation will not be processed as either Chat or User file is missing or there are no rows in it\")        \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Merging Email and Chat data\")\n",
    "    df_communication_stats = df_email_stats.merge(df_chat_stats, how='outer', left_on=['interaction_from','Date'], right_on=['chat_id','chat_date'])\n",
    "    df_communication_stats.interaction_from = df_communication_stats.interaction_from.fillna(df_communication_stats.chat_id)\n",
    "    df_communication_stats.Date = df_communication_stats.Date.fillna(df_communication_stats.chat_date)\n",
    "    df_communication_stats.drop(['chat_id','chat_date'], axis=1, inplace=True)\n",
    "    df_communication_stats = df_communication_stats.fillna(0)\n",
    "    logging.info(str(pd.datetime.now()) + \": Merging of Email and Chat data is complete\")\n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Data Processing ,cleanup and feature generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    audio_policy_cols = extract_policy_columns(audio)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    def audio_dataset_cleanup(dataset):\n",
    "        columns = list(dataset.columns)\n",
    "        interaction_sources = ['NiceTradingRecording', 'NTRExport', 'NIM', 'Avaya', 'Verint', 'InterCall']\n",
    "        dataset = dataset[dataset.interaction_source.isin(interaction_sources)]\n",
    "        for col in columns:\n",
    "            if dataset[col].isnull().any()==True:\n",
    "                if col in ('interaction_start', 'interaction_original_author_id'):\n",
    "                    dataset = dataset.dropna(axis=0, subset=[col])\n",
    "                if col in ('interaction_direction'):\n",
    "                    dataset[col] = dataset[col].fillna('Outgoing')\n",
    "                if col in ('interaction_language'):\n",
    "                    dataset[col]  = dataset.groupby(\"interaction_from\")[col].transform(lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else \"UNK\")\n",
    "                if col in ('interaction_type', 'interaction_source'):\n",
    "                    dataset[col] = dataset[col].fillna('UNK')\n",
    "                else:    \n",
    "                    dataset[col] = dataset[col].fillna(0)\n",
    "        dataset = dataset[(dataset.interaction_start.str.contains('T') == True)]\n",
    "        dataset['audio_interaction_start']=dataset.interaction_start.map(lambda x: x.split('|')[0].strip() if x.find('|') != -1 else x.strip())\n",
    "        dataset['audio_date']=dataset.audio_interaction_start.map(lambda x: x.split('T')[0].strip())\n",
    "        dataset['audio_hour']=dataset.interaction_start.map(lambda x: int(x.split('T')[1].strip().split(':')[0]))\n",
    "        dataset['audio_minutes']=dataset.interaction_start.map(lambda x: int(x.split('T')[1].strip().split(':')[1]))\n",
    "        dataset['audio_time']=dataset.audio_hour*60 + dataset.audio_minutes\n",
    "        dataset['audio_Non_English_flag']=dataset.interaction_language.map(lambda x: 0 if x in ('English','International English') else 1 if pd.notnull(x) else 0)\n",
    "        #Seperating the original_author_id for Outgoing Interaction direction\n",
    "        outgoing_audio_data = dataset[(dataset['interaction_direction'] == 'Outgoing') | (dataset['interaction_direction'] == 'Internal')]\n",
    "        outgoing_audio_data = outgoing_audio_data[['interaction_sourceid', 'interaction_original_author_id']]\n",
    "        #outgoing_audio_data = outgoing_audio_data[outgoing_audio_data.interaction_original_author_id.str.contains('\\|') == True]\n",
    "        outgoing_audio_data['interaction_from']=outgoing_audio_data.interaction_original_author_id.map(lambda x: x.split('|').pop(0) if x.find('|') != -1 else x)\n",
    "        outgoing_audio_data['interaction_to']=outgoing_audio_data.interaction_original_author_id.map(lambda x: x.split('|')[1:len(x.split('|'))] if x.find('|') != -1 else 'UNK')\n",
    "        outgoing_audio_data['interaction_to']=outgoing_audio_data.interaction_to.map(lambda x: \"~\".join(a for a in x) if x != 'UNK' else 'UNK')\n",
    "        incoming_audio_data = dataset[dataset['interaction_direction'] == 'Incoming']\n",
    "        incoming_audio_data = incoming_audio_data[['interaction_sourceid', 'interaction_original_author_id']]\n",
    "        incoming_audio_data['interaction_from']='UNK'\n",
    "        #incoming_audio_data = incoming_audio_data[incoming_audio_data.interaction_original_author_id.str.contains('\\|') == True]\n",
    "        incoming_audio_data['interaction_to']=incoming_audio_data.interaction_original_author_id.map(lambda x: x.split('|')[0:len(x.split('|'))] if x.find('|') != -1 else 'UNK')\n",
    "        incoming_audio_data['interaction_to']=incoming_audio_data.interaction_to.map(lambda x: \"~\".join(a for a in x) if x != 'UNK' else 'UNK')\n",
    "        audio_dataset=pd.concat([outgoing_audio_data, incoming_audio_data])\n",
    "        dataset=pd.merge(dataset, audio_dataset, left_on='interaction_sourceid', right_on='interaction_sourceid')\n",
    "        del dataset['interaction_from_x']\n",
    "        del dataset['interaction_to_x']\n",
    "        del dataset['interaction_original_author_id_x']\n",
    "        dataset=dataset.rename(columns={'interaction_from_y': 'interaction_from', 'interaction_original_author_id_y' : 'interaction_original_author_id','interaction_to_y':'interaction_to'})\n",
    "        return dataset    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    def audio_from_user_mapping(dataset):\n",
    "        #users.Avaya_EUSID=users.Avaya_EUSID.astype('str')\n",
    "        try:\n",
    "            NTR_df = dataset[dataset.interaction_source == 'NiceTradingRecording']\n",
    "            NTR_users = users[['Exchange_EUSID', 'NiceTradingRecording_EUSID']]\n",
    "            NTR_df = NTR_df.merge(NTR_users, how='left', left_on='interaction_from', right_on='NiceTradingRecording_EUSID')\n",
    "            NTR_df.drop('NiceTradingRecording_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"NiceTradingRecording_EUSID is missing in user file\")\n",
    "            #NTR_df = NTR_df.merge(NTR_users, how='left', left_on='interaction_to', right_on='NiceTradingRecording_EUSID')\n",
    "            #NTR_df.drop('NiceTradingRecording_EUSID',axis=1,inplace=True)\n",
    "        try:\n",
    "            Avaya_df = dataset[dataset.interaction_source == 'Avaya']\n",
    "            Avaya_users = users[['Exchange_EUSID', 'Avaya_EUSID']]\n",
    "            Avaya_users.Avaya_EUSID = Avaya_users.Avaya_EUSID.astype('str')\n",
    "            Avaya_df.interaction_from = Avaya_df.interaction_from.astype('str')\n",
    "            Avaya_df = Avaya_df.merge(Avaya_users, how='left', left_on='interaction_from', right_on='Avaya_EUSID')\n",
    "            Avaya_df.drop('Avaya_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"Avaya_EUSID is missing in user file\")\n",
    "        try:    \n",
    "            NTRExport_df = dataset[dataset.interaction_source == 'NTRExport']\n",
    "            NTRExport_users = users[['Exchange_EUSID', 'NTRExport_EUSID']]\n",
    "            NTRExport_df = NTRExport_df.merge(NTRExport_users, how='left', left_on='interaction_from', right_on='NTRExport_EUSID')\n",
    "            NTRExport_df.drop('NTRExport_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"NTRExport_EUSID is missing in user file\")        \n",
    "        try:    \n",
    "            NIM_df = dataset[dataset.interaction_source == 'NIM']\n",
    "            NIM_users = users[['Exchange_EUSID', 'NIM_EUSID']]\n",
    "            NIM_df = NIM_df.merge(NIM_users, how='left', left_on='interaction_from', right_on='NIM_EUSID')\n",
    "            NIM_df.drop('NIM_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"NIM_EUSID is missing in user file\")    \n",
    "        try:\n",
    "            Verint_df = dataset[dataset.interaction_source == 'Verint']\n",
    "            Verint_users = users[['Exchange_EUSID', 'Verint_EUSID']]\n",
    "            Verint_df = Verint_df.merge(Verint_users, how='left', left_on='interaction_from', right_on='Verint_EUSID')\n",
    "            Verint_df.drop('Verint_EUSID', axis=1, inplace=True)    \n",
    "        except:\n",
    "            print(\"Verint_EUSID is missing in user file\") \n",
    "        try:\n",
    "            Intercall_df = dataset[dataset.interaction_source == 'InterCall']\n",
    "            Intercall_users = users[['Exchange_EUSID', 'Intercall_EUSID']]\n",
    "            Intercall_df = Intercall_df.merge(Intercall_users, how='left', left_on='interaction_from', right_on='Intercall_EUSID')\n",
    "            Intercall_df.drop('Intercall_EUSID', axis=1, inplace=True)    \n",
    "        except:\n",
    "            print(\"Intercall_EUSID is missing in user file\") \n",
    "            #Merging the datasets from different sources into one\n",
    "            #dataset = pd.concat([NTR_df,Avaya_df,Folder_df,NIM_df], axis=0)\n",
    "        dataset = pd.concat([NTR_df, NTRExport_df, Avaya_df, NIM_df, Verint_df, Intercall_df], axis=0)\n",
    "        dataset.Exchange_EUSID = dataset.Exchange_EUSID.fillna(dataset.interaction_from)\n",
    "            #dataset.Exchange_EUSID_y = dataset.Exchange_EUSID_y.fillna(dataset.interaction_to)\n",
    "        del dataset['interaction_from']\n",
    "        del dataset['interaction_to']\n",
    "        dataset=dataset.rename(columns={'Exchange_EUSID': 'interaction_from'})#,'Exchange_EUSID_y' : 'interaction_to'})\n",
    "        return dataset    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    class audio_sent_feature_generation:\n",
    "        def window_selection(dataset, window):\n",
    "            if window == 'daily':\n",
    "                duration_feature='audio_date'\n",
    "            if window == 'weekly':\n",
    "                duration_feature='week_of_year'\n",
    "            if window == 'monthly':\n",
    "                duration_feature='month_of_year'\n",
    "            if window == 'quarter':\n",
    "                duration_feature = 'quarter_of_year'\n",
    "            dataset = dataset.rename(columns={'interaction_from' : 'users'})\n",
    "            grouped_data = dataset.groupby(['users', duration_feature])\n",
    "            def call_sent(dataset):\n",
    "                call_sent = dataset.interaction_sourceid.nunique().reset_index(name='Call_Made')\n",
    "                return call_sent\n",
    "            def sum_call_sent_duration(dataset):\n",
    "                sum_call_sent_duration = dataset['interaction_duration_int'].sum().reset_index(name=\"sum_call_sent_duration\")\n",
    "                return sum_call_sent_duration\n",
    "            def count_call_sent_duration(dataset):\n",
    "                count_call_sent_duration = dataset['interaction_duration_int'].count().reset_index(name=\"count_call_sent_duration\")\n",
    "                return count_call_sent_duration\n",
    "            def sum_call_sent_paricipants(dataset):\n",
    "                sum_call_sent_paricipants = dataset['participant_count_BF_I'].sum().reset_index(name=\"sum_call_sent_paricipants\")\n",
    "                return sum_call_sent_paricipants\n",
    "            def count_call_sent_paricipants(dataset):\n",
    "                count_call_sent_paricipants = dataset['participant_count_BF_I'].count().reset_index(name=\"count_call_sent_paricipants\")\n",
    "                return count_call_sent_paricipants\n",
    "            def sum_call_sent_time(dataset):\n",
    "                sum_call_sent_time = dataset['audio_time'].sum().reset_index(name=\"sum_call_sent_time\")\n",
    "                return sum_call_sent_time\n",
    "            def count_call_sent_time(dataset):\n",
    "                count_call_sent_time = dataset['audio_time'].count().reset_index(name=\"count_call_sent_time\")\n",
    "                return count_call_sent_time            \n",
    "            def non_english_made_call(dataset):\n",
    "                non_english_made_call = dataset.audio_Non_English_flag.sum().reset_index(name=\"NonEnglish_Call_Made\")\n",
    "                for col in audio_policy_cols:\n",
    "                    non_english_made_call['voice_' + col] = dataset[col].mean().values                \n",
    "                return non_english_made_call\n",
    "            call_sent = call_sent(grouped_data)\n",
    "            sum_call_sent_duration = sum_call_sent_duration(grouped_data)\n",
    "            count_call_sent_duration = count_call_sent_duration(grouped_data)\n",
    "            sum_call_sent_paricipants = sum_call_sent_paricipants(grouped_data)\n",
    "            count_call_sent_paricipants = count_call_sent_paricipants(grouped_data)\n",
    "            sum_call_sent_time = sum_call_sent_time(grouped_data)\n",
    "            count_call_sent_time = count_call_sent_time(grouped_data)\n",
    "            non_english_made_call = non_english_made_call(grouped_data)\n",
    "            sent_feature = pd.merge(call_sent, sum_call_sent_duration, how='outer', on=['users',duration_feature])\n",
    "            dataframe_list = [count_call_sent_duration,\n",
    "                            sum_call_sent_paricipants,\n",
    "                            count_call_sent_paricipants,\n",
    "                            sum_call_sent_time,\n",
    "                            count_call_sent_time,\n",
    "                            non_english_made_call]\n",
    "                            #non_english_recieve_call]\n",
    "            for i in dataframe_list:\n",
    "                sent_feature = pd.merge(sent_feature, i, how='outer', on=['users', duration_feature])\n",
    "            return sent_feature         \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    def splitting_audio_recieved_users(dataset):\n",
    "        recieve_data = dataset[['interaction_sourceid', 'interaction_to']]\n",
    "        df_interaction_to = recieve_data.interaction_to.str.split('~', expand=True)\n",
    "        recieved_names_df = pd.DataFrame(columns=['interaction_sourceid', 'interaction_to'])\n",
    "        list1 = df_interaction_to.values.tolist()\n",
    "        for index,value in enumerate(list1):\n",
    "            a = list(recieve_data.iloc[index:index+1,0])\n",
    "            b = list(filter(None, list1[index]))\n",
    "            index = pd.MultiIndex.from_product([a, b], names=['interaction_sourceid', 'interaction_to'])\n",
    "            names_df = pd.DataFrame(index = index).reset_index()\n",
    "            recieved_names_df = pd.concat([recieved_names_df, names_df], axis=0)\n",
    "            recieved_names_df.reset_index(drop=True)\n",
    "        del dataset['interaction_to']\n",
    "        dataset=pd.merge(dataset, recieved_names_df, left_on='interaction_sourceid', right_on='interaction_sourceid')\n",
    "        #dataset=dataset[['interaction_sourceid','interaction_to','audio_date','interaction_direction','audio_time','audio_Non_English_flag','participant_count_BF_I','interaction_duration_int','interaction_source','interaction_language']]\n",
    "        #dataset=pd.merge(dataset,audio_dataset,left_on='interaction_sourceid', right_on='interaction_sourceid')\n",
    "        return dataset\n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def audio_to_user_mapping(dataset):\n",
    "        #users['Avaya_EUSID']=users['Avaya_EUSID'].astype('str')\n",
    "        try:\n",
    "            NTR_df = dataset[dataset.interaction_source == 'NiceTradingRecording']\n",
    "            NTR_users = users[['Exchange_EUSID', 'NiceTradingRecording_EUSID']]\n",
    "            #NTR_df = NTR_df.merge(NTR_users, how='left', left_on='interaction_from', right_on='NiceTradingRecording_EUSID')\n",
    "            #NTR_df.drop('NiceTradingRecording_EUSID',axis=1,inplace=True)\n",
    "            NTR_df = NTR_df.merge(NTR_users, how='left', left_on='interaction_to', right_on='NiceTradingRecording_EUSID')\n",
    "            NTR_df.drop('NiceTradingRecording_EUSID',axis=1,inplace=True)\n",
    "        except:\n",
    "            print(\"NiceTradingRecording_EUSID is missing in user file\")\n",
    "        try:\n",
    "            Avaya_df = dataset[dataset.interaction_source=='Avaya']\n",
    "            Avaya_users = users[['Exchange_EUSID', 'Avaya_EUSID']]\n",
    "            Avaya_users.Avaya_EUSID = Avaya_users.Avaya_EUSID.astype('str')\n",
    "            Avaya_df.interaction_to = Avaya_df.interaction_to.astype('str')\n",
    "            Avaya_df = Avaya_df.merge(Avaya_users, how='left', left_on='interaction_to', right_on='Avaya_EUSID')\n",
    "            Avaya_df.drop('Avaya_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"Avaya_EUSID is missing in user file\")\n",
    "        try:    \n",
    "            NTRExport_df = dataset[dataset.interaction_source == 'NTRExport']\n",
    "            NTRExport_users = users[['Exchange_EUSID', 'NTRExport_EUSID']]\n",
    "            NTRExport_df = NTRExport_df.merge(NTRExport_users, how='left', left_on='interaction_from', right_on='NTRExport_EUSID')\n",
    "            NTRExport_df.drop('NTRExport_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"NTRExport_EUSID is missing in user file\")            \n",
    "        try:\n",
    "            NIM_df = dataset[dataset.interaction_source == 'NIM']\n",
    "            NIM_users = users[['Exchange_EUSID', 'NIM_EUSID']]\n",
    "            NIM_df = NIM_df.merge(NIM_users, how='left', left_on='interaction_to', right_on='NIM_EUSID')\n",
    "            NIM_df.drop('NIM_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"NIM_EUSID is missing in user file\")\n",
    "        try:\n",
    "            Verint_df = dataset[dataset.interaction_source == 'Verint']\n",
    "            Verint_users = users[['Exchange_EUSID', 'Verint_EUSID']]\n",
    "            Verint_df = Verint_df.merge(Verint_users, how='left', left_on='interaction_to', right_on='Verint_EUSID')\n",
    "            Verint_df.drop('Verint_EUSID', axis=1, inplace=True)\n",
    "        except:\n",
    "            print(\"Verint_EUSID is missing in user file\")\n",
    "        try:\n",
    "            Intercall_df = dataset[dataset.interaction_source == 'InterCall']\n",
    "            Intercall_users = users[['Exchange_EUSID', 'Intercall_EUSID']]\n",
    "            Intercall_df = Intercall_df.merge(Intercall_users, how='left',left_on='interaction_to', right_on='Intercall_EUSID')\n",
    "            Intercall_df.drop('Intercall_EUSID', axis=1, inplace=True)   \n",
    "        except:\n",
    "            print(\"Intercall_EUSID is missing in user file\")\n",
    "        #Merging the datasets from different sources into one\n",
    "        #dataset = pd.concat([NTR_df,Avaya_df,Folder_df,NIM_df], axis=0)\n",
    "        dataset = pd.concat([NTR_df, NTRExport_df, Avaya_df, NIM_df, Verint_df, Intercall_df], axis=0)\n",
    "        dataset.Exchange_EUSID = dataset.Exchange_EUSID.fillna(dataset.interaction_to)\n",
    "        #dataset.Exchange_EUSID_y = dataset.Exchange_EUSID_y.fillna(dataset.interaction_to)\n",
    "        del dataset['interaction_from']\n",
    "        del dataset['interaction_to']\n",
    "        dataset=dataset.rename(columns={'Exchange_EUSID': 'interaction_to'})#,'Exchange_EUSID_y' : 'interaction_to'})\n",
    "        return dataset    \n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    class audio_recieve_feature_generation:\n",
    "        def window_selection(dataset,window):\n",
    "            if window == 'daily':\n",
    "                duration_feature = 'audio_date'\n",
    "            if window == 'weekly':\n",
    "                duration_feature = 'week_of_year'\n",
    "            if window == 'monthly':\n",
    "                duration_feature='month_of_year'\n",
    "            if window == 'quarter':\n",
    "                duration_feature = 'quarter_of_year'\n",
    "            dataset=dataset.rename(columns={'interaction_to':'users'})  \n",
    "            grouped_data=dataset.groupby(['users', duration_feature])\n",
    "            def call_recieved(dataset):\n",
    "                call_recieved=dataset.interaction_sourceid.nunique().reset_index(name='Call_Recieved')\n",
    "                return call_recieved\n",
    "            def sum_call_recieve_duration(dataset):\n",
    "                sum_call_recieve_duration=dataset['interaction_duration_int'].sum().reset_index(name=\"sum_call_recieve_duration\")\n",
    "                return sum_call_recieve_duration\n",
    "            def count_call_recieve_duration(dataset):\n",
    "                count_call_recieve_duration=dataset['interaction_duration_int'].count().reset_index(name=\"count_call_recieve_duration\")\n",
    "                return count_call_recieve_duration\n",
    "            def sum_call_recieve_paricipants(dataset):\n",
    "                sum_call_recieve_paricipants=dataset['participant_count_BF_I'].sum().reset_index(name=\"sum_call_recieve_paricipants\")\n",
    "                return sum_call_recieve_paricipants\n",
    "            def count_call_recieve_paricipants(dataset):\n",
    "                count_call_recieve_paricipants=dataset['participant_count_BF_I'].count().reset_index(name=\"count_call_recieve_paricipants\")\n",
    "                return count_call_recieve_paricipants\n",
    "            def sum_call_recieve_time(dataset):\n",
    "                sum_call_recieve_time=dataset['audio_time'].sum().reset_index(name=\"sum_call_recieve_time\")\n",
    "                return sum_call_recieve_time\n",
    "            def count_call_recieve_time(dataset):\n",
    "                count_call_recieve_time=dataset['audio_time'].count().reset_index(name=\"count_call_recieve_time\")\n",
    "                return count_call_recieve_time\n",
    "            def non_english_recieve_call(dataset):\n",
    "                non_english_recieve_call=dataset.audio_Non_English_flag.sum().reset_index(name=\"NonEnglish_Call_Receive\")\n",
    "                return non_english_recieve_call     \n",
    "            #call_sent=call_sent(dataset,duration_feature)\n",
    "            call_recieved = call_recieved(grouped_data)\n",
    "            sum_call_recieve_duration = sum_call_recieve_duration(grouped_data)\n",
    "            count_call_recieve_duration = count_call_recieve_duration(grouped_data)\n",
    "            sum_call_recieve_paricipants = sum_call_recieve_paricipants(grouped_data)\n",
    "            count_call_recieve_paricipants = count_call_recieve_paricipants(grouped_data)\n",
    "            sum_call_recieve_time = sum_call_recieve_time(grouped_data)\n",
    "            count_call_recieve_time = count_call_recieve_time(grouped_data)\n",
    "            non_english_recieve_call = non_english_recieve_call(grouped_data)\n",
    "            recieve_feature = pd.merge(call_recieved,sum_call_recieve_duration, how='outer', on=['users',duration_feature])\n",
    "            dataframe_list = [count_call_recieve_duration,\n",
    "                            sum_call_recieve_paricipants,\n",
    "                            count_call_recieve_paricipants,\n",
    "                            sum_call_recieve_time,\n",
    "                            count_call_recieve_time,\n",
    "                            non_english_recieve_call]\n",
    "            for i in dataframe_list:\n",
    "                recieve_feature=pd.merge(recieve_feature, i, how='outer', on=['users',duration_feature])\n",
    "            return recieve_feature        \n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    def voice_chat_email_aggregation(audio):        \n",
    "        col_list = ['interaction_from', 'interaction_to', 'interaction_original_author_id', 'interaction_author']        \n",
    "        for col in col_list:\n",
    "            audio[col] = audio[col].astype('str').apply(lambda x: x.lower())\n",
    "        audio_df = audio_dataset_cleanup(audio)\n",
    "        sent_audio_df = audio_from_user_mapping(audio_df)\n",
    "        df_sent_audio_stats = audio_sent_feature_generation.window_selection(sent_audio_df,'daily')\n",
    "        to_audio_df = splitting_audio_recieved_users(audio_df)\n",
    "        recieve_audio_df = audio_to_user_mapping(to_audio_df)\n",
    "        df_recieve_audio_stats = audio_recieve_feature_generation.window_selection(recieve_audio_df,'daily')\n",
    "        df_audio_stats = df_sent_audio_stats.merge(df_recieve_audio_stats, how='outer', left_on=['users', 'audio_date'], right_on=['users', 'audio_date'])\n",
    "        #df_audio_stats = df_audio_stats['users'].astype('object')\n",
    "        #df_audio_stats = df_audio_stats['audio_date'].astype('object')\n",
    "        df_audio_stats = df_audio_stats[df_audio_stats['users'] != 'UNK']\n",
    "        df_audio_stats = df_audio_stats.fillna(0)\n",
    "        return df_audio_stats    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Voice data cleaning and Aggregation in progress\")\n",
    "    if 'voice' in file_list and 'user' in file_list:\n",
    "        df_audio_stats = voice_chat_email_aggregation(audio)\n",
    "        logging.info(str(pd.datetime.now()) + \": Voice data cleaning and Aggregation is complete\")\n",
    "    else:\n",
    "        df_audio_stats = pd.DataFrame(columns=['users', 'audio_date', 'Call_Made', 'sum_call_sent_duration',\n",
    "                                               'count_call_sent_duration', 'sum_call_sent_paricipants',\n",
    "                                               'count_call_sent_paricipants', 'sum_call_sent_time',\n",
    "                                               'count_call_sent_time', 'NonEnglish_Call_Made', 'Call_Recieved',\n",
    "                                               'sum_call_recieve_duration', 'count_call_recieve_duration',\n",
    "                                               'sum_call_recieve_paricipants', 'count_call_recieve_paricipants',\n",
    "                                               'sum_call_recieve_time', 'count_call_recieve_time',\n",
    "                                               'NonEnglish_Call_Receive'] , dtype='object')\n",
    "        logging.info(str(pd.datetime.now()) + \": Voice Cleaning and aggregation will not be processed as either Voice or User file is missing or there are no rows in it\")        \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    logging.info(str(pd.datetime.now()) + \": Combining Email, chat and voice data\")\n",
    "    df_communication_stats = df_communication_stats.merge(df_audio_stats, how='outer', left_on=['interaction_from','Date'], right_on=['users','audio_date'])\n",
    "    df_communication_stats.interaction_from = df_communication_stats.interaction_from.fillna(df_communication_stats.users)\n",
    "    df_communication_stats.Date = df_communication_stats.Date.fillna(df_communication_stats.audio_date)\n",
    "    df_communication_stats.drop(['users', 'audio_date'], axis=1, inplace=True)\n",
    "    df_communication_stats.dropna(subset=['interaction_from'], inplace=True)\n",
    "    df_communication_stats = df_communication_stats[df_communication_stats['interaction_from']!='']\n",
    "    df_communication_stats = df_communication_stats.fillna(0)\n",
    "    logging.info(str(pd.datetime.now()) + \": Email, chat and voice combined successfully\")\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Merging observe policy columns for email chat and voice\")\n",
    "    for i, col in enumerate(list(set(email_policy_cols + chat_policy_cols + audio_policy_cols))):\n",
    "        f_col = []\n",
    "        for col1 in df_communication_stats.columns:\n",
    "            if (col == col1[5:] or col == col1[6:]):\n",
    "                f_col.append(col1)\n",
    "        df_communication_stats[col] = df_communication_stats[f_col].sum(axis=1)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Droping observe policy related unwanted columns \")\n",
    "    df_communication_stats.drop([col for col in df_communication_stats.columns if col.startswith(\"email_interaction_risk_\") or col.startswith(\"chat_interaction_risk_\") or col.startswith(\"voice_interaction_risk_\")], axis=1, inplace=True)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Merging observe policy columns for email chat and voice\")\n",
    "    for i, col in enumerate(list(set(email_policy_cols + chat_policy_cols + audio_policy_cols))):\n",
    "        f_col = []\n",
    "        for col1 in df_communication_stats.columns:\n",
    "            if (col == col1[5:] or col == col1[6:]):\n",
    "                f_col.append(col1)\n",
    "        df_communication_stats[col] = df_communication_stats[f_col].sum(axis=1)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Droping observe policy related unwanted columns \")\n",
    "    df_communication_stats.drop([col for col in df_communication_stats.columns if col.startswith(\"email_interaction_risk_\") or col.startswith(\"chat_interaction_risk_\") or col.startswith(\"voice_interaction_risk_\")], axis=1, inplace=True)\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt(col):\n",
    "    col = col.astype(str)\n",
    "    df_col = pd.DataFrame(col)\n",
    "    encrypted_col = df_col.applymap(lambda x: f_key.encrypt(x.encode('utf-8')))\n",
    "    return encrypted_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Transfer the aggregated data files to S3\")\n",
    "    if df_communication_stats.shape[0] > 0:        \n",
    "        df_communication_stats_encrypted = encrypt(df_communication_stats['interaction_from'])\n",
    "        df_communication_stats.interaction_from = df_communication_stats_encrypted\n",
    "        bytes_to_write = df_communication_stats.to_csv(index=False).encode()\n",
    "        start_date = datetime.strptime(min(df_communication_stats['Date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "        end_date = datetime.strptime(max(df_communication_stats['Date']), '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "        processed_date = datetime.now().date().strftime('%Y-%m-%d')\n",
    "        processed_file = processed_path + '/Input_Data_Aggregated_' + processed_date + '|' + start_date + '|' + end_date +'.csv'\n",
    "        s3 = s3fs.S3FileSystem(anon=False)\n",
    "        with s3.open(processed_file, 'wb') as f:\n",
    "            f.write(bytes_to_write)\n",
    "    else:\n",
    "        logging.info(str(pd.datetime.now()) + \": Warning- No data to write\")        \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(str(pd.datetime.now()) + \": Deleting the source data files from S3\")\n",
    "    path_list = [email_file_path,chat_file_path,audio_file_path]\n",
    "    for path in path_list:\n",
    "        try:\n",
    "            fs.rm(path, recursive=True)\n",
    "        except:\n",
    "            logging.info(str(pd.datetime.now()) + ': ' + path.split('/')[6] + ' file not found')            \n",
    "    logging.info(str(pd.datetime.now()) + \": File deletion Complete from S3\")    \n",
    "except:   \n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))\n",
    "    log_file_to_s3()\n",
    "    shutdown_notebook_instance()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log_file_to_s3()\n",
    "except:\n",
    "    logging.info(str(pd.datetime.now()) + ': ERROR- ' + str(sys.exc_info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !find . -maxdepth 1 -type f -name '*DataAggregationLog*' -not -name \"*$t*\" -delete\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutdown_notebook_instance()\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
