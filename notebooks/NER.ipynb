{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 37.671811915934086}\n",
      "Statring iteration 1\n",
      "{'ner': 14.328150227040169}\n",
      "Statring iteration 2\n",
      "{'ner': 7.4484561562101135}\n",
      "Statring iteration 3\n",
      "{'ner': 4.681439850603035}\n",
      "Statring iteration 4\n",
      "{'ner': 4.361068695255446}\n",
      "Statring iteration 5\n",
      "{'ner': 3.739919171371057}\n",
      "Statring iteration 6\n",
      "{'ner': 3.141550442246047}\n",
      "Statring iteration 7\n",
      "{'ner': 2.3460405052646127}\n",
      "Statring iteration 8\n",
      "{'ner': 1.4204411031160769}\n",
      "Statring iteration 9\n",
      "{'ner': 0.375257840903572}\n",
      "Statring iteration 10\n",
      "{'ner': 0.025826251115337512}\n",
      "Statring iteration 11\n",
      "{'ner': 0.0014656770193084165}\n",
      "Statring iteration 12\n",
      "{'ner': 0.0007853349934323473}\n",
      "Statring iteration 13\n",
      "{'ner': 8.556096765119438e-05}\n",
      "Statring iteration 14\n",
      "{'ner': 1.9883477674832654e-05}\n",
      "Statring iteration 15\n",
      "{'ner': 1.5425055296783718e-06}\n",
      "Statring iteration 16\n",
      "{'ner': 6.437957801205007e-08}\n",
      "Statring iteration 17\n",
      "{'ner': 1.3706293093243885e-07}\n",
      "Statring iteration 18\n",
      "{'ner': 5.080363012587313e-08}\n",
      "Statring iteration 19\n",
      "{'ner': 1.171063051801752e-08}\n",
      "Enter your Model Name: ner1\n",
      "Enter your testing text: I love Patna\n",
      "love 2 6 PrdName\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "import spacy\n",
    "import random\n",
    "\n",
    "\n",
    "TRAIN_DATA = [('London, Patna, delhi', {'entities': [(0, 6, 'GEO'),(8, 13, 'GEO'),(15, 20, 'GEO')]}), ('what is the price of desktop?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of bottle?', {'entities': [(21, 27, 'PrdName')]}), ('what is the price of mouse?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of keyboad?', {'entities': [(21, 28, 'PrdName')]}), ('what is the price of chair?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of table?', {'entities': [(21, 26, 'PrdName')]}), ('what is the price of watch?', {'entities': [(21, 26, 'PrdName')]})]\n",
    "\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "prdnlp = train_spacy(TRAIN_DATA, 20)\n",
    "\n",
    "# Save our trained Model\n",
    "modelfile = input(\"Enter your Model Name: \")\n",
    "prdnlp.to_disk(modelfile)\n",
    "\n",
    "#Test your text\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "doc = prdnlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patna 7 12 GEO\n",
      "London 14 20 GEO\n",
      "polio 33 38 GEO\n",
      "Peris 40 45 GEO\n",
      "Pune 47 51 GEO\n",
      "Anku 53 57 GEO\n",
      "very 59 63 GEO\n"
     ]
    }
   ],
   "source": [
    "doc = prdnlp('Why is Patna, London is price of polio, Peris, Pune, Anku, very is')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = input(\"Enter your train data filename : \")\n",
    "print(filename)\n",
    "\n",
    "\n",
    "with open(filename) as train_data:\n",
    "\ttrain = json.load(train_data)\n",
    "\n",
    "TRAIN_DATA = []\n",
    "for data in train:\n",
    "\tents = [tuple(entity) for entity in data['entities']]\n",
    "\tTRAIN_DATA.append((data['content'],{'entities':ents}))\n",
    "\n",
    "\n",
    "with open('{}'.format(filename.replace('json','txt')),'w') as write:\n",
    "\twrite.write(str(TRAIN_DATA))\n",
    "\n",
    "print('-------------Copy and Paste to spacy training-------------')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(TRAIN_DATA)\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print('--------------------------End-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.1.0\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-23-389e1a7b3278>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-389e1a7b3278>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),)\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "output_dir=None\n",
    "n_iter=100\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 13, 'LOC')\n",
      "(18, 24, 'LOC')\n",
      "(7, 17, 'PERSON')\n"
     ]
    }
   ],
   "source": [
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4.838125703216065}\n",
      "Losses {'ner': 5.422199606895447}\n",
      "Losses {'ner': 6.114254844840616}\n",
      "Losses {'ner': 5.696460766717792}\n",
      "Losses {'ner': 6.095897443592548}\n",
      "Losses {'ner': 3.5277975655626506}\n",
      "Losses {'ner': 5.572458281822037}\n",
      "Losses {'ner': 4.292205314151943}\n",
      "Losses {'ner': 3.055013504199451}\n",
      "Losses {'ner': 2.6746739973787044}\n",
      "Losses {'ner': 3.948901522442611}\n",
      "Losses {'ner': 4.965447286143899}\n",
      "Losses {'ner': 5.898705250738287}\n",
      "Losses {'ner': 7.248114375048317}\n",
      "Losses {'ner': 2.6854841281310655}\n",
      "Losses {'ner': 3.684295305836713}\n",
      "Losses {'ner': 1.0365400400260114}\n",
      "Losses {'ner': 3.392805339593906}\n",
      "Losses {'ner': 1.789718436961266}\n",
      "Losses {'ner': 3.1891572810709476}\n",
      "Losses {'ner': 2.941910615663801}\n",
      "Losses {'ner': 5.891705294678104}\n",
      "Losses {'ner': 3.350152664896086}\n",
      "Losses {'ner': 2.4088675418270213}\n",
      "Losses {'ner': 0.0932395051640924}\n",
      "Losses {'ner': 3.2696622627620044}\n",
      "Losses {'ner': 2.713374140133965}\n",
      "Losses {'ner': 1.59421786483108}\n",
      "Losses {'ner': 0.009197531349855126}\n",
      "Losses {'ner': 2.4299662364646792}\n",
      "Losses {'ner': 2.6999423497718453}\n",
      "Losses {'ner': 2.9107633856428947}\n",
      "Losses {'ner': 0.9576108891105832}\n",
      "Losses {'ner': 0.044292416889220476}\n",
      "Losses {'ner': 0.11153108204872514}\n",
      "Losses {'ner': 2.090747343629573}\n",
      "Losses {'ner': 1.9558322917347937}\n",
      "Losses {'ner': 0.40271934670816467}\n",
      "Losses {'ner': 0.02614642005613687}\n",
      "Losses {'ner': 0.18334655227583418}\n",
      "Losses {'ner': 0.01450588092882299}\n",
      "Losses {'ner': 0.0001058100206370316}\n",
      "Losses {'ner': 0.004764946638830203}\n",
      "Losses {'ner': 0.028203202313619613}\n",
      "Losses {'ner': 0.00014707706706647283}\n",
      "Losses {'ner': 0.0006769342186792038}\n",
      "Losses {'ner': 0.0008666019791044866}\n",
      "Losses {'ner': 0.00039574412918738644}\n",
      "Losses {'ner': 0.002089022398578777}\n",
      "Losses {'ner': 0.00014326359793115362}\n",
      "Losses {'ner': 0.0005198465026484819}\n",
      "Losses {'ner': 0.002104920969941304}\n",
      "Losses {'ner': 5.024776498683235e-07}\n",
      "Losses {'ner': 0.0357573227026356}\n",
      "Losses {'ner': 0.0018927776124057494}\n",
      "Losses {'ner': 0.002282262857644657}\n",
      "Losses {'ner': 6.228897279333539e-09}\n",
      "Losses {'ner': 0.016855305068642655}\n",
      "Losses {'ner': 1.710225704919611}\n",
      "Losses {'ner': 3.417589385539422e-06}\n",
      "Losses {'ner': 4.974944480653999e-05}\n",
      "Losses {'ner': 3.8509703662593085e-08}\n",
      "Losses {'ner': 0.0004369152641388041}\n",
      "Losses {'ner': 1.1256269358175808e-05}\n",
      "Losses {'ner': 8.45550629869924e-06}\n",
      "Losses {'ner': 0.0004850015070722199}\n",
      "Losses {'ner': 0.00016695682739609197}\n",
      "Losses {'ner': 7.807713783720511e-05}\n",
      "Losses {'ner': 0.03557768964868521}\n",
      "Losses {'ner': 0.0004144279563482241}\n",
      "Losses {'ner': 0.020603588328157098}\n",
      "Losses {'ner': 8.578721444604895e-07}\n",
      "Losses {'ner': 3.0041839880950235e-08}\n",
      "Losses {'ner': 0.0022472221145126703}\n",
      "Losses {'ner': 2.7221692546877773e-07}\n",
      "Losses {'ner': 0.00012244425676831544}\n",
      "Losses {'ner': 1.4549778114951156e-09}\n",
      "Losses {'ner': 7.066481222655491e-08}\n",
      "Losses {'ner': 7.878335170810943e-05}\n",
      "Losses {'ner': 1.6988681874909052e-05}\n",
      "Losses {'ner': 1.3051922007689958e-05}\n",
      "Losses {'ner': 8.351041730675675e-07}\n",
      "Losses {'ner': 2.2838221825504163e-05}\n",
      "Losses {'ner': 1.0407691488494052e-07}\n",
      "Losses {'ner': 4.193246648726608e-09}\n",
      "Losses {'ner': 5.204189902955575e-07}\n",
      "Losses {'ner': 8.235155950563862e-06}\n",
      "Losses {'ner': 0.00012083178484542285}\n",
      "Losses {'ner': 3.0868022274899285e-08}\n",
      "Losses {'ner': 1.8492697194934384e-08}\n",
      "Losses {'ner': 4.832108796148926e-07}\n",
      "Losses {'ner': 4.1660199125381097e-10}\n",
      "Losses {'ner': 5.531067309294665e-07}\n",
      "Losses {'ner': 1.6877982712880466e-08}\n",
      "Losses {'ner': 2.33391936921567e-05}\n",
      "Losses {'ner': 4.415604756616005e-08}\n",
      "Losses {'ner': 5.583045599820461e-10}\n",
      "Losses {'ner': 5.9626077693427715e-05}\n",
      "Losses {'ner': 1.539056808688798e-05}\n",
      "Losses {'ner': 3.1953888253243145e-07}\n"
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    # reset and initialize the weights randomly – but only if we're\n",
    "    # training a new model\n",
    "    if model is None:\n",
    "        nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        print(\"Losses\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to C:\\Users\\amrenkumar\\Desktop\\TestData\n"
     ]
    }
   ],
   "source": [
    "# save model to output directory\n",
    "output_dir =r'C:\\Users\\amrenkumar\\Desktop\\TestData'\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from C:\\Users\\amrenkumar\\Desktop\\TestData\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp2(text)\n",
    "    print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n",
      "Tokens [('May', '', 2), ('is', '', 2), ('the', '', 2), ('hottest', '', 2), ('month', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp2(\"May is the hottest month\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [model] [output_dir] [n_iter]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.1.0\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int)\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
